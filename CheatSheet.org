#+TITLE: Reference Sheet for Elementary Category Theory
# SUBTITLE: Cheat Sheet Template
# DATE: << Spring 2018 >>
# When we don't provide a date, one is provided for us.
#+AUTHOR: [[http://www.cas.mcmaster.ca/~alhassm/][Musa Al-hassy]] @@latex:{\tiny\hspace{5.5em}\url{https://github.com/alhassy/CatsCheatSheet}}@@
#+EMAIL: alhassy@gmail.com
#+DESCRIPTION: This document is written by Musa Al-hassy for his learning in the spring of 2018.
#+STARTUP: hideblocks
#+STARTUP: overview
#+TODO: TODO { BEGIN-IGNORE(b) END-IGNORE(e) } | DONE(d)

# Important shortcuts:
# f7 preview changes
# f8 commit each change
# f9 push changes

#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \newunicodechar{﹔}{\ensuremath{\raisebox{0.4ex}{\tiny \,;\,}}}  %% forward composition ﹔

#+LATEX_HEADER: \usepackage{calculation} 

#+INCLUDE: ~/Dropbox/MyUnicodeSymbols.org
#+INCLUDE: CheatSheet/CheatSheetSetup.org

* TODO commit info                                                 :ignore:

#+begin_tiny
  \vspace{-3.5em}
  \hspace{19em} [[https://github.com/alhassy/CatsCheatSheet/blob/984969814031f56f53732419f297e668d94df863/CheatSheet.pdf][~commit 9849698~]]
  \vspace{2em}
#+end_tiny

* LaTeX Setup :ignore:

#+BEGIN_EXPORT latex
\def\providedS{ \qquad\Leftarrow\qquad }

\def\impliesS{ \qquad\Rightarrow\qquad }

\def\landS{ \qquad\land\qquad }
\def\lands{ \quad\land\quad }

\def\eqs{ \quad=\quad}

\def\equivs{ \quad\equiv\quad}
\def\equivS{ \qquad\equiv\qquad}

\def\begineqns{ \begingroup\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt} }
\def\endeqns{ \endgroup }
% \def\endeqns{ \endgroup \setlength{\belowdisplayskip}{2pt} } % put belowspace back to desired setting
#+END_EXPORT

# See defn-Type, below for an expanded usage; \eqn{name}{formula}
# LaTeX: \setlength{\abovedisplayskip}{5pt} \setlength{\belowdisplayskip}{2pt}
#+LaTeX: \def\eqn#1#2{ \begin{flalign*} #2 && \tag*{#1} \label{#1} \end{flalign*}  }

#+LaTeX: \def\src{\mathsf{src}}
#+LaTeX: \def\tgt{\mathsf{tgt}}
#+LaTeX: \def\Id{\mathsf{Id}}
#+LaTeX: \def\bin{I\!\!I}

# LATEX_HEADER: \setlength{\parskip}{1em}
# LaTeX: \setlength{\parskip}{0.5em}

#+LaTeX: \def\room{\vspace{0.5em}}

* Categories

A *category* 𝒞 consists of a collection of “objects” ~Obj 𝒞,~ a collection of
  “morphisms” ~Mor 𝒞~, an operation ~Id~ associating a morphism ~Idₐ : a → a~ to each object ~a~,
  a parallel pair of functions ~src, tgt : Mor 𝒞 → Obj 𝒞~, and a “composition”
  operation ~_﹔_ : ∀{A B C : Obj} → (A → B) → (B → C) → (A → C)~
  where for objects ~X~ and ~Y~ we define the /type/ ~X → Y~
  as follows
\begin{flalign*}
    f : X \to Y \quad\equiv\quad \mathsf{src}\; f = X \;\land\; \mathsf{tgt}\; f = Y 
   &&
   \tag*{type-Definition}
   \label{type-Definition}
\end{flalign*}
 
 Moreover composition is required to be associative with ~Id~ as identity.
# For other approaches see https://tex.stackexchange.com/a/12035/69371
#
# As we can see from \eqref{defn-Type}\ldots
#
#  \vspace{-1em}

Instead of ~src~ and ~tgt~ we can instead assume primitive a ternary relation
~_:_→_~ and regain the operations precisely when the relation is functional
in its last two arguments:
\eqn{type-Unique}{f : A \to B \;\land\; f : A' \to B' \;\implies\; A=A' \;\land\; B=B'}
When this condition is dropped, we obtain a /pre-category/; e.g., the familiar /Sets/
is a pre-category that is usually treated as a category by making morphisms
contain the information about their source and target: ~(A, f, B) : A → B~
rather than just ~f~. 
\newline
 /This is sometimes easier to give, then src and tgt! C.f. Alg(F)./

\room

Here's an equivalence-preserving property that is useful in algebraic calculations,
#+LaTeX: \eqn{Composition}{ f : A → B \lands g : B → A \equivs f﹔g : A → A \lands g﹔f : B → B}

# A categorical statement is an expression built from notations for objects,
# typing, morphisms, composition, and identities by means of the usual logical
# connectives and quantifications and equality.
#

Example Categories.
+ Each digraph determines a category: The objects are the nodes
  and the paths are the morphisms typed with their starting and ending node.
  Composition is catenation of paths and identity is the empty path.
+ Each preorder determines a category: The objects are the elements
  and there is a morphism ~a → b~ named, say, ~(a, b),~ precisely when $a \leq b$.

\room

Even when morphisms are functions, the objects need not be sets:
Sometimes the objects are /operations/ --with an appropriate definition
of typing for the functions. The categories of F-algebras are an example
of this.


\newpage

* Functors

A *functor* /F : 𝒜 → ℬ/ is a pair of mappings, denoted by one name,
from the objects, and morphisms, of 𝒜 to those of ℬ such that
it respects the categorical structure:

#+BEGIN_EXPORT latex 
{\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

\eqn{functor-Type}{F\, f : F\, A \to_ℬ F\, B \quad\Leftarrow\quad f : A \to_𝒜 B }

\eqn{Functor}{F\, \Id_A \;=\; \Id_{F\, A}} 

\eqn{Functor}{F\, (f ﹔ g) \;=\; F\, f ﹔ F\, g}

}
#+END_EXPORT

\vspace{1em}

The two axioms are equivalent to the single statement that 
/functors distribute over finite compositions, with $\Id$ being the empty composition/
\[ F(f ﹔ \cdots ﹔ g) \;=\; F\, f ﹔ \cdots ﹔ F\, g \]

Use of Functors.
+ In the definition of a category, “objects” are “just things” for which no internal
  structure is observable by categorical means --composition, identities, morphisms, typing.

  Functors form the tool to deal with “structured” objects

  Indeed in 𝒮ℯ𝓉 the aspect of a structure is that it has “constituents”, and that it is possible
  to apply a function to all the individual constituents; this is done by
  /F f : F A → F B/.

+  For example, let $\bin A = A × A$ and $\bin f = (x, y) ↦ (f\, x, f\, y)$.
  So $\bin$ is or represents the structure of pairs; $\bin\, A$ is the set of pairs of /A/,
  and $\bin\, f$ is the function that applies /f/ to each constituent of a pair.

  - A /binary operation on A/ is then just a function $\bin A → A$;
    in the same sense we obtain /F-ary operations/.

+ Also, /Seq/ is or represents the structure of sequences; /Seq A/ is the structure of sequences
  over /A/, and /Seq f/ is the function that applies /f/ to each constituent of a sequence.

+  Even though /F A/ is still just an object, a thing with no observable internal structure, the
  functor properties enable to exploit the “structure” of /F A/ by allowing us to “apply”
  a /f/ to each “constituent” by using /F f/.

\vspace{1em}

Category $𝒜lℊ(F)$
+ For a functor /F : 𝒜 → 𝒟/, this category has /F-algebras/, /F/-ary operations in 𝒟 as, objects
  -- i.e., objects are 𝒟-arrows $F\, A → A$ --
  and /F/-homomorphisms as morphisms, and it inherits composition and identities from 𝒟.

  #+BEGIN_EXPORT latex 
  {\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

  \eqn{defn-Homomorphism}{f : ⊕ →_F ⊗ \quad\equiv\quad ⊕ ﹔ f = F\, f ﹔ ⊗ }

  \eqn{id-Homomorphism}{ \Id : ⊕ →_F ⊕ }

  \eqn{comp-Homomorhism}{ f ﹔ g : ⊕ →_F ⊙ \qquad\Leftarrow\qquad f : ⊕ →_F ⊗ \;\land\; g : ⊗ →_F ⊙}
  }
  #+END_EXPORT

  Note that category axiom \eqref{unique-Type} is not fulfilled since a function can be
  a homomorphism between several distinct operations. However, we pretend it is a category
  in the way discussed earlier, and so the carrier of an algebra is fully determined by
  the operation itself, so that the operation itself can be considered the algebra.

  #+BEGIN_CENTER
  /\ref{comp-Homomorhism} renders a semantic property as a syntactic condition!/
  #+END_CENTER

\vspace{1em}

+ A *contravariant functor* 𝒞 → 𝒟 is just a functor /𝒞ᵒᵖ → 𝒟ᵒᵖ/.
+ A *bifunctor* from 𝒞 to 𝒟 is just a functor /𝒞² → 𝒟/.

* Naturality

A natural transformation is nothing but a structure preserving map between functors.
“Structure preservation” makes sense, here, since we've seen already that a functor
is, or represents, a structure that objects might have.

\room

As discussed before for the case /F : 𝒞 → 𝒮ℯ𝓉/, each /F A/ denotes a structured set
and /F/ denotes the structure itself.

# \room
#
#+LaTeX: \paragraph{\footnotesize \textnormal{Example Structures}}\label{SeqPair-is-Pair-Seq}
\hspace{-1em}:
$\bin$ is the structure of pairs, /Seq/ is the structure of sequences,
/Seq Seq/ the structure of sequences of sequences, 
$\bin \, Seq$ the structure of pairs of sequences --which is naturally isomorphic
to $Seq \, \bin$ the structure of sequences of pairs!--, and so on.

\room

A “transformation” from structure /F/ to structure /G/ is a family of functions \newline
$η : ∀\{A\} → F\, A → G\, A$; and it is “natural” if each $η_A$ doesn't affect the /constituents/
of the structured elements in /F A/ but only reshapes the structure of the elements,
from an /F/-structure to a /G/-structure.

\vspace{0em}

#+BEGIN_CENTER
/Reshaping the structure by η commutes with subjecting the constituents to an arbitrary morphism./
#+END_CENTER
# That is, $F\, f ﹔ t_B \;=\; t_A ﹔ G\, f$ for all /f : A → B./

\vspace{-2em}
#+LaTeX: \eqn{ntrf-Def}{ η : F →̣ G \quad\equiv\quad ∀ f \,•\, F\, f ﹔ η_{\tgt\, f}\;=\; η_{\src\, f} ﹔ G\, f }

This is `naturally' remembered: Morphism $η_{\tgt\, f}$ has type $F (\tgt\, f) → G(\tgt\, f)$ and therefore
appears at the target side of an occurrence of /f/; similarly $η_{\src\, f}$ occurs at the source side of an /f/.
/Moreover/ since η is a transformation /from/ /F/ to /G/, functor /F/ occurs at the source side of an η
and functor /G/ at the target side.

\room

+ One also says /ηₐ is natural in/ its parameter /a/.

+ If we take $G = \Id$, then natural transformations $F →̣ \Id$ are precisely /F/-homomorphisms.
+ Indeed, a natural transformation is a sort-of homomorphism in that the image of a morphism
  after reshaping is the same as the reshaping of the image.

\room

Example natural transformations
+ /rev : Seq →̣ Seq : [a₁, …, aₙ] ↦ [aₙ, …, a₁]/
  reverses its argument thereby reshaping a sequence structure into a sequence structure without affecting the constituents.

+ /inits : Seq →̣ Seq Seq : [a₁, …, aₙ] ↦ [[], [a₁], ⋯, [a₁, …, aₙ]]/
  yields all initial parts of its argument
  thereby reshaping a sequence structure into a sequence of sequences structure, not affecting
  the constituents of its argument.

\room

#+BEGIN_EXPORT latex
\begineqns

\eqn{ntr-Ftr}{ Jη : JF →̣ JG \providedS η : F →̣ G \quad \text{ where } (Jη)_A ≔ J(η_A) }

\eqn{ntr-Poly}{ ηK : FK →̣ GK  \hspace{-2ex}\providedS η : F →̣ G \quad \text{ where } (ηK)_A ≔ η_{(K\, A) } }

\eqn{ntrf-Id}{ \Id_F : F →̣ F \text{\hspace{12em} where } (\Id_F)_A ≔ \Id_{(F\, A)} }

\eqn{ntrf-Compose}{ ε ﹔ η : F →̣ H \hspace{2ex}\providedS ε : F →̣ G \lands η : G →̣ H 
\\ \text{ where } (ε ﹔ η)_A = ε_A ﹔ η_A
 }

\endeqns
#+END_EXPORT

\room

*Category ℱ𝓊𝓃𝒸(𝒞, 𝒟)*
consists of functors /𝒞 → 𝒟/ as objects and natrual transformations between them as objects.
The identity transformation is indeed an identity for transformation composition, which is associative. 

\room

*Heuristic* To prove $φ = φ₁ ﹔ ⋯ ﹔ φₙ : F →̣ G$ is a natural transformation, it suffices
to show that each $φᵢ$ is a natural transformation.
 + Theorem \eqref{ntrf-Compose} renders proofs of semantic properties to be trivial type checking!
 + E.g., It's trivial to prove /tails = rev ﹔ inits ﹔ Seq rev/ is a natural transformation
   by type checking, but to prove the naturality equation by using the naturality equations of
   /rev/ and /inits/ --no definitions required-- necessitates more writing, and worse: Actual thought! 

* Adjunctions

An adjunction is a particular one-one correspondence between different kinds of
morphisms in different categories.

\room

An *adjunction* consists of two functors $L : 𝒜 → ℬ$ and $R : ℬ → 𝒜$,
as well as two (not necessarily natural!) transformations
$η : \Id → RL$ and $ε : LR → \Id$ such that

\vspace{-1em}

#+BEGIN_EXPORT latex
\eqn{Adjunction}{\text{\tiny Provided $f : A →_𝒜 R B$ and $g : L A →_ℬ B$ }
 \\ f = η_A ﹔ R g \equivS L f ﹔ ε_B = g
}
#+END_EXPORT

Reading right-to-left: In the equation $L f ﹔ ε_B = g$ there is a unique solution to the unknown $f$.
Dually for the other direction.

\room

That is,
/each L-algebra g is uniquely determined --as an L-map followed by an ε-reduce--/
/by its restriction to the adjunction's unit η./

\room

A famous example is “Free ⊣ Forgetful”, e.g. to /define/ the list datatype, for which the above
becomes: Homomorphisms on lists are uniquely determined, as a map followed by a reduce,
by their restriction to the singleton sequences.

\room

We may call $f$ the restriction, or lowering, of $g$ to the “unital case”
and write $f = ⌊g⌋ = η_A ﹔ R g$. Also, we may call $g$ the extension, or raising,
of $f$ to an /L/-homomorphism and write $g = ⌈f⌉ = L f ﹔ ε_B$. The above equivalence
now reads:

#+BEGIN_EXPORT latex
\begineqns

\eqn{Adjunction-Inverse}{f = ⌊g⌋ \equivS ⌈f⌉ = g}

\eqn{lad-Type}{⌊g⌋_{A,B} = η_A ﹔ R g \; : \; A →_𝒜 R B
 \text{ where } g : L A →_ℬ B
 }

\eqn{rad-Type}{⌈f⌉_{A,B} = L f ﹔ ε_B \; : \; L A →_ℬ B
 \text{ where } f : A →_𝒜 R B
 }

\endeqns
#+END_EXPORT

\room
\vspace{1ex} 
Note that ⌈ is like `r' and the argument to ⌈⌉ must involve the /R/-ight adjoint in its type;
# likewise for 
#+LaTeX: {\textbf L}ad takes morphisms involving the {\textbf L}eft adjoint ;)

\room

This equivalence expresses that `lad' $⌊⌋$, from \emph{l}eft \emph{ad}jungate,
and `rad' $⌈⌉$, from \emph{r}ight \emph{ad}jungate, are each other's inverses
and constitute a correspondence between certain morphisms.
/Being a bijective pair, lad and rad are injective, surjective, and undo one another./

\room

We may think of ℬ as having all complicated problems so we abstract
away some difficulties by \emph{r}aising up to a cleaner, simpler, domain
via rad ⌈⌉; we then solve our problem there, then go back \emph{down} to
the more complicated concrete issue via ⌊⌋, lad.
\newline
( E.g., ℬ is the category of monoids, and 𝒜 is the category of sets; $L$ is the list functor. )

# Some useful results,
#+BEGIN_EXPORT latex
\begineqns

\eqn{ntrf-Adj}{\text{The η and ε determine each other and they are \emph{natural} transformations.}}

\vspace{2ex}
“zig-zag laws” The unit has a post-inverse while the counit has a pre-inverse:

\eqn{unit-Inverse}{ \Id = η   ﹔ R ε}

\eqn{Inverse-counit}{ \Id = L η ﹔ ε} 

\vspace{2ex}
The unit and counit can be regained from the adjunction inverses,

\eqn{unit-Def}{ η = ⌊\Id⌋}

\eqn{counit-Def}{ ε = ⌈\Id⌉ }

\vspace{2ex}
Lad and rad themselves are solutions to the problems of interest, \eqref{Adjunction}.

\eqn{lad-Self}{L ⌊g⌋ ﹔ ε = g}

\eqn{rad-Self}{η ﹔ R ⌈f⌉ = f }

\vspace{2ex}
The following laws assert a kind of
monoic-ness for ε and a kind of epic-ness for η.
Pragmatically they allow us to prove an equality
by shifting to a possibly easier equality obligation.

\eqn{lad-Unique}{ η ﹔ R g = η ﹔ R g′ \equivS g = g′}

\eqn{rad-Unique}{L f ﹔ ε \,= L f′ ﹔ ε \,\equivS f = f′}

\vspace{2ex}
Lad and rad are natural transformations in the category $ℱ𝓊𝓃𝒸(𝒜ᵒᵖ × ℬ, 𝒮ℯ𝓉)$ realising
$(L X → Y) ≅ (X → G Y)$ where $X, Y$ are the first and second projection functors
and $(-→-) : 𝒞ᵒᵖ × 𝒞 → 𝒮ℯ𝓉$ is the hom-functor such that $(f → g) h = f ﹔ h ﹔ g$.
\\ By extensionality in 𝒮ℯ𝓉, their naturality amounts to the laws:

\eqn{lad-Fusion}{⌊ L x ﹔ g ﹔   y ⌋ \;=\; x   ﹔ ⌊g⌋ ﹔ R y }

\eqn{rad-Fusion}{⌈   x ﹔ f ﹔ R y ⌉ \;=\; L x ﹔ ⌈f⌉ ﹔   y }

\room
\endeqns
#+END_EXPORT

Also,
+ Left adjoints preserve colimits such as initial objects and sums.
+ Right adjoints preserve limits such as terminal objects and products.

* Skolemisation
# “Up to Isomorphism”

If a property $P$ holds for precisely one class of isomorphic objects,
and for any two objects in the same class there is precisely one
isomorphism from one to the other, then we say that
/the P-object is unique up to unique isomorphism/. 
For example, in 𝒮ℯ𝓉 the one-point set is unique up to a unique isomorphism,
but the two-point set is not.

\room

For example, an object /A/ is ``initial'' iff
$∀ B  \;•\;  ∃₁ f  \;•\;  f : A → B$, and such objects are unique
up to unique isomorphism --prove it!
The formulation of the definition is clear but it's not very well suited for /algebraic manipulation/.

\room

A convenient formulation is obtained by `skolemisation': An assertion of the form
\[ ∀ x \;•\; ∃₁ y \;•\; R \, x \, y \]
is equivalent to: There's a function ℱ such that
\[ ∀ x, y \;•\; R \, x \, y \;≡\; y = ℱ\, x  \]
In the former formulation it is the existential quantification “$∃ y$” inside the scope of a universal
one that hinders effective calculation. In the latter formulation the existence claim is brought to a
more global level: A reasoning need no longer be interrupted by the declaration and naming of the
existence of a unique $y$ that depends on $x$; it can be denoted just $ℱ\, x$.
As usual, the final universal quantification can be omitted, thus simplifying the formulation once more.

\room

In view of the important role of the various $y$'s, these $y$'s deserve a particular notation that
triggers the reader of their particular properties. We employ bracket notation such as $⦇ x ⦈$
for such $ℱ\, x$: An advantage of the bracket notation is that no extra parentheses are needed
for composite arguments $x$, which we expect to occur often.

\room

The formula /characterising/ $ℱ$ may be called `ℱ-Char' and it immediately give us some results
by truthifying each side, namely `Self' and `Id'. A bit more on the naming:

| Type        | Possibly non-syntactic constraint on notation being well-formed |
| Self        | It, itself, is a solution                                       |
| Id          | How $\Id$ can be expressed using it                             |
| Uniq        | It's problem has a unique solution                              |
| Fusion      | How it behaves wrt composition                                  |
| Composition | How two instances, in full subcategories, compose               |

Note that the last 3 indicate how the concept interacts with the categorical structure:
$=, ﹔, \Id$. Also note that Self says there's at least one solution and Uniq says there is
at most one solution, so together they are equivalent to ℱ-Char --however those two proofs
are usually not easier nor more elegant than a proof of ℱ-Char directly.

* Initiality

# Convenient definition: 
An object /0 is initial/ if there's a mapping $⦇-⦈$, from objects to morphisms,
such that \ref{initial-Char} holds; from which we obtain a host of useful corollaries.
Alternative notations for $⦇ B ⦈$ are $\text{!`}_B$, or $⦇0 → B⦈$ to make the
dependency on 0 explicit.

#+BEGIN_EXPORT latex 
\begineqns

\eqn{initial-Char}{ f : 0 → B \equivS f = ⦇ B ⦈ }

\eqn{initial-Self}{ ⦇ B ⦈ : 0 → B }

\eqn{initial-Id}{ id₀ = ⦇ 0 ⦈ }

\eqn{initial-Uniq}{ f, g : 0 → B \impliesS f = g }

\eqn{initial-Fusion}{ f : B → C \impliesS ⦇ B ⦈ ﹔ f = ⦇ C ⦈ }

\vspace{2ex}
{\tiny Provided objects $B, C$ are both in 𝒜 and ℬ,
which are full subcategories of some category 𝒞:}
\eqn{initial-Compose}{ ⦇A → B⦈_𝒜 ﹔ ⦇ B → C ⦈_ℬ = ⦇ A → C⦈_𝒜 }
%
% Recall: 𝒟 is a full-subcategory of 𝒞 means: 𝒟(x,y) = 𝒞(x,y) for x,y : Obj 𝒟.

\vspace{2ex}
{\tiny Provided 𝒟 is built on top of 𝒞: 𝒟-objects are composite entities in 𝒞 } 
\eqn{initial-Type}{ B \text{ is an object in 𝒟 } \impliesS ⦇ B ⦈ \text{ is a morphism in 𝒞} }

\endeqns
#+END_EXPORT

\vspace{1em}

These laws become much more interesting when the category is built upon another
one and the typing is expressed as one or more equations in the underlying
category. In particular the importance of fusion laws cannot be over-emphasised;
it is proven by a strengthening step of the form
$⦇B⦈ ﹔ f : 0 → C \providedS ⦇B⦈ : 0 → B \lands f : B → C$.

\room

For example, it can be seen that the datatype of sequences is `the' initial object
in a suitable category, and the mediator $⦇-⦈$ captures
“definitions by induction on the structure”! Hence induction arguments
can be replaced by initiality arguments! Woah!

\room

*Proving Initiality* One may prove that an object $0$ is initial by providing
a definition for $⦇-⦈$ and establishing initial-Char. Almost every such
proof has the following format, or a circular implication thereof: For arbitrary /f/ and /B/,

\vspace{2em}
#+begin_calculation latex
    f : A → B
\step[≡]{}
    ⋮
\step[≡]{}
    f = \text{“an expression not involving $f$”}
\step[≡]{ 𝒹ℯ𝒻𝒾𝓃ℯ $⦇ B ⦈$ to be the rhs of the previous equation }
    f = ⦇ B ⦈
#+end_calculation

\vspace{-2em}

* Colimits

#+LaTeX: \def\const#1{ \underline{#1} }

Each colimit is a certain initial object, and each initial object is a certain colimit.

+ A /diagram in 𝒞/ is a functor $D : 𝒟 → 𝒞$.
+ Define the constant functor $\const{C}\, x = C$ for objects $x$ and
  $\const{C}\, f = \Id_C$ for morphisms $f$. For functions $g : A → B$, we define the natural
  transformation $\const{g} = x \mapsto g : \const{A} →̣ \const{B}$.

+ The category $⋁D$, built upon 𝒞, has objects $γ : D →̣ \const{C}$ called “co-cones”, for
  some object $C =: \tgt\, γ$, and a morphism from $γ$ to $δ$ is a 𝒞-morphism $x$ such that $γ ﹔ \const{x} = δ$.
 
+ A /colimit for D/ is an initial object in $⋁ D$; which may or may not exist.

\room

Writing $γ╲-$ for $⦇-⦈$ and working out the definition of co-cone in terms of equations in 𝒞,
  we obtain: /$γ : Obj(⋁D)$ is a colimit for $D$/ if there is a mapping $γ╲-$ such that /╲-Type/ and
/╲-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\backslash$-Type}{ δ \text{ cocone for } D \impliesS γ╲δ : \tgt\, γ → \tgt\,δ}

\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of $\backslash$-Type is met.
The notation $⋯╲δ$ is only senseful if δ is a co-cone for $D$,
like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

\eqn{$\backslash$-Char}{ γ ﹔ \const{x} = δ \equivS x = γ ╲ δ }

\vspace{2ex}
Notice that for given $x : C → C'$ the equation $γ ╲ δ = x$ \underline{defines}
δ, since by $\backslash$-Char that one equation equivales the family of equations
$δ_A = γ_A ﹔ x$. This allows us to define a natural transformation --or `eithers' in
the case of sums-- using a single function \emph{having} the type of the mediating arrow.

\eqn{$\backslash$-Self}{ γ ; \const{γ╲δ} = δ}

\eqn{$\backslash$-Id}{γ╲γ = \Id}

\eqn{$\backslash$-Fusion}{γ╲δ ﹔x = γ╲(δ ﹔ \const{x})}
% Direct Proof:
%
%    γ╲δ ﹔ x = γ ╲ (δ ﹔ _x_)
% ≡  γ ﹔ \const{γ╲δ ﹔ x} = δ ﹔ _x_          ╲-char
% ≡  γ ﹔ \const{γ╲δ} ﹔ _x_ = δ ﹔ _x_          const functor
% ≡  δ ﹔ _x_ = δ ﹔ _x_                         ╲-self
% ≡ true                                        =-reflexivity

\eqn{$\backslash$-Unique}{γ ﹔ \const{x} = γ ﹔ \const{y} \impliesS x = y}

\vspace{2ex}
This expresses that colimits γ have an epic-like property: \\
The component morphisms $γ_A$ are \emph{jointly epic}.

\vspace{2ex}
The following law confirms the choice of notation once more.

\eqn{$\backslash$-Compose}{γ╲δ ﹔ δ╲ε = γ╲ε}

\vspace{2ex}
The next law tells us that functors distribute over the ╲-notation
provided the implicit well-formedness condition that 
$Fγ$ is a colimit holds --clearly this condition is valid when $F$
preserves colimits.

\eqn{$\backslash$-Functor-Dist}{F(γ╲δ) = Fγ ╲ Fδ}

\eqn{$\backslash$-Pre-Functor-Elim}{γF╲δF = γ╲δ}

\endeqns
#+END_EXPORT


\vspace{-1em}

* Limits

Dually, the category $⋀D$ has objects being “cones” $γ : \const{C} →̣ D$ where $C =: \src\, γ$
is a 𝒞-object, and a morphism to $γ$ /from/ $δ$ is a 𝒞-morphism $x$ such that $\const{x} ﹔ γ = δ$.
In terms of 𝒞, /$γ : Obj(⋀ D)$ is a limit for $D$/ if there is a mapping $γ╱-$ such that
the following ╱-Type and ╱-Char hold, from which we obtain a host of corollaries.
As usual, there is the implicit well-formedness condition. 
\ref{/-Unique} expresses that limits γ have an monic-like property:
The component morphisms $γ_A$ are \emph{jointly monic}.
#
# Well-formedness convention: In each law the variables are quantified
# in such a way that the premise of $/$-Type is met.
# The notation $δ╱⋯$ is only senseful if δ is a cone for $D$,
# like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

#+BEGIN_EXPORT latex
\begineqns

\eqn{/-Type}{ δ \text{ cone for } D \impliesS δ╱γ : \src\, δ → \src\,γ}

\eqn{/-Char}{ \const{x} ﹔ γ = δ \equivS x = δ ╱ γ }

\eqn{/-Self}{ \const{δ ╱ γ} ﹔ γ = δ}

\eqn{/-Id}{γ╱γ = \Id}

\eqn{/-Fusion}{x ﹔ δ ╱ γ = (\const{x} ﹔ δ) ╱ γ}

\eqn{/-Unique}{\const{x} ﹔ γ = \const{y} ﹔ γ \impliesS x = y}

\eqn{/-Functor-Dist}{F(δ ╱ γ) = Fδ ╱ Fγ}

\eqn{/-Pre-Functor-Elim}{δF╱γF = δ╱γ}
% Direct proof:
%
%    δF╱γF = δ╱γ
% ≡  δF = (δ ╱ γ) ﹔ γF
% ≡  (δ ╱ γ ﹔ γ)F = (δ ╱ γ) ﹔ γF
% ≡  (δ ╱ γ ﹔ γ)FA = ((δ ╱ γ) ﹔ γF)A
% ≡  δ ╱ γ ; γFA  = δ╱γ ﹔ γFA
% ≡  true
% 
\endeqns
#+END_EXPORT


\vspace{-1em}

* Coequaliser

# $D𝒟 = \left( \overset{A}{•} \overset{\overset{f}{\rightrightarrows}}{g} \overset{B}{•} \right)$.
#
Take $D$ and $𝒟$ as suggested by $D𝒟 = \left( \overset{A}{•} \rightrightarrows^f_g \overset{B}{•} \right)$;
where $f,g : A → B$ are given. Then a cocone δ for $D$ is a two-member family $δ = (q', q)$
with $q' : A → C, q : B → C, C = \tgt\,\delta$ and $\const{C} h ﹔ δ_A = δ_B ﹔ D h$; in-particular
$q' = q ﹔ f = g ﹔ q$ whence $q'$ is fully-determined by $q$ alone.

Let $γ = (p', p) : Obj(⋁D)$ be a colimit for $D$ and write $p╲-$ in-place of $γ╲-$, then the ╲-laws
yield: /$p$ is a coequaliser of $(f,g)$/ if there is a mapping $p╲-$ such that /CoEq-Type/ and
/CoEq-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{CoEq-Type}{ q ﹔ f =  q ﹔ g \impliesS p╲q : \tgt\, p → \tgt\,q}


\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of \ref{CoEq-Type} is met.
The notation $⋯╲q$ is only senseful if $q﹔f=q﹔g$,
like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

\eqn{CoEq-Char}{ p ﹔ x = q \equivS x = p ╲ q }

\eqn{CoEq-Self}{ p ; p╲q = q}

\eqn{CoEq-Id}{p╲p = \Id}

\eqn{CoEq-Fusion}{p╲q ﹔x = p╲(q ﹔ x)}

\eqn{CoEq-Unique}{p ﹔ x = p ﹔ y \impliesS x = y}

\eqn{CoEq-Compose}{p╲q ﹔ q╲r = p╲r}
%%
%% \eqn{?╲-Functor-Dist}{F(γ╲δ) = Fγ ╲ Fδ}
%% 
%% \eqn{?╲-Pre-Functor-Elim}{γF╲δF = γ╲δ}
%%
\endeqns
#+END_EXPORT

\newpage
* Sums
#+LaTeX: \def\inl{\mathsf{inl}} 
#+LaTeX: \def\inr{\mathsf{inr}}

Take $D$ and $𝒟$ as suggested by $D𝒟 = \left( \overset{A}{•} \;\;\; \overset{B}{•} \right)$.
Then a cocone δ for $D$ is a two-member family $δ = (f, g)$ with
$f : A → C$ and $g : B → C$, where $C = \tgt\, δ$.

\room

Let $γ=(\inl, \inr)$ be a colimit for $D$, let $A + B = \tgt\,γ$, and write
$[f, g]$ in-place of $γ╲(f, g)$, then the ╲-laws yield:
/$(\inl, \inr, A+B)$ form a sum of $A$ and $B$/ if there is a mapping $[-,-]$
such that \ref{[]-Type} and \ref{[]-Char} hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{[]-Type}{f : A → C \lands g : B → C \impliesS [f, g] : A + B → C}

\eqn{[]-Char}{ \inl ﹔ x = f \lands \inr ﹔ x = g \equivS x = [f, g] }

\eqn{[]-Cancellation; []-Self}{ \inl ﹔ [f, g] = f \landS \inr ﹔ [f, g] = g}

\eqn{[]-Id}{ [\inl, \inr] = \Id}

\eqn{[]-Unique}{ \inl ﹔ x = \inl ﹔ y \lands \inr ﹔ x = \inr ﹔ y \impliesS x = y}

\eqn{[]-Fusion}{ [f , g] ﹔ x = [f﹔ x, g ﹔ x] }

\vspace{2ex}
The implicit well-formedness condition in the next law is that
$(F\,\inl, F\,\inr, F(A+B))$ form a sum of $F\,A$ and $F\, B$.
%  --which clearly holds if $F$ preserves sums.

\eqn{[]-Functor-Dist}{F \, [f, g]_𝒞 = [F \, f , F \, g]_𝒟 \qquad\text{ where } F : 𝒞 → 𝒟}
%
% f : A → C
% g : B → C
% F[f,g]     : F(A + B) → F C
% [F f, F g] : F A + F B → F C
%
% Types are okay since, by assumption, F(A + B) forms the sum of FA and FB,
% That is, (F A + F B) ≔ F(A + B)
%
% Direct proof: 
% 
%    F[f,g] = [Ff, Fg]
% ≡  F[f,g] ﹔ F inl = F f  ∧ F[f,g] ﹔ F inr = F g
% ≡  F([f,g] ﹔ inl) = F f  ∧ F([f,g] ﹔ inr) = F g
% ⇐ [f,g] ﹔ inl = f  ∧ [f,g] ﹔ inr = g
% ≡  [f,g] = [f,g]
% ≡  true

\endeqns
#+END_EXPORT

* Duality: Sums & Products

#+BEGIN_EXPORT latex
\def\fst{\mathsf{fst}}
\def\snd{\mathsf{snd}}
#+END_EXPORT

#+LaTeX_HEADER: \usepackage{stackengine} 
#
# \topinset{*}{O}{1pt}{}
#
#+LaTeX: \def\composition{ \topinset{﹔}{∘}{0.4pt}{} }

In category theory there are two popular notations for composition,
$f ﹔ g = g ∘ f$, and there are two arrow notations $A → B \;=\; B \leftarrow A$,
known as the “forwards” and “backwards” notations.

\room

Some people prefer one notation and stick with it; however having both in-hand
allows us to say: The /dual/ of a categorical statement formed with 
$﹔,→$ is obtained by syntactically replacing these two with $∘, \leftarrow$ respectively
while leaving variables and $\Id$'s alone.

\room

#+LaTeX: \def\dual{\mathsf{dual}}
#
#
For example, applying this process to sums yields /products/:
\vspace{-0.5em}
#+begin_calculation latex 
   h = ⟨f, g⟩
\step{  We define products as dual to sums }
   h = \dual [f, g]
\step{  dual operation definition }
  \dual\left( h = [f,g] \right)
\step{  []-Char and Leibniz }
  \dual\left( \inl ﹔ h = f  \lands  \inr ﹔ h = g\right) 
\step{  dual operation definition }
  \dual\left(\inl ﹔ h\right) = f  \lands  \dual\left(\inr ﹔ h\right) = g) 
\step{  dual operation definition }
  \dual \text{ } \inl ∘ h = f  \lands  \dual \text{ }  \inr ∘ h = g
\step{  Define: $\fst = \dual\, \inl$, $\snd = \dual\, \inr$ }
  \fst ∘ h = f  \lands  \snd ∘ h = g
\step{  Switch back to ﹔-notation }
   h ﹔ \fst = f  \lands  h ﹔ \snd = g
#+end_calculation

# Dualising the other sum artefacts yields:
/$(\fst, \snd, A × B)$ form a product of A and B/ if there is an operation
$⟨-,-⟩$ satisfying the Char and Type laws below; from which we obtain a host of corollaries.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\langle\rangle$-Type}{f : C → A \lands g : C → B \impliesS ⟨f, g⟩ : C → A × B}

\eqn{$\langle\rangle$-Char}{ x ﹔ \fst = f \lands x ﹔ \snd = g \equivS x = ⟨f, g⟩ }

\eqn{$\langle\rangle$-Cancellation; $\langle\rangle$-Self}{ ⟨f, g⟩ ﹔ \fst = f \landS ⟨f, g⟩ ﹔ \snd = g}

\eqn{$\langle\rangle$-Id}{ ⟨\fst, \snd⟩ = \Id}

\eqn{$\langle\rangle$-Unique}{ x ﹔ \fst = y ﹔\fst  \lands x; \snd = y ﹔ \snd \impliesS x = y}

\eqn{$\langle\rangle$-Fusion}{ x ﹔ ⟨f , g⟩ = ⟨x﹔f, x ﹔ g⟩ }

\eqn{$\langle\rangle$-Functor-Dist}{F \, ⟨f, g⟩_𝒞 = ⟨F \, f , F \, g⟩_𝒟 \qquad\text{ where } F : 𝒞 → 𝒟}

\endeqns
#+END_EXPORT

\room

These are essentially a re-write of the sum laws; let's write the next set of laws only once.

\room

Let the tuple “⟅ ⟆, ⋆, $l$, $r$, $\composition$” be either
“⟨ ⟩, ×, $\fst$, $\snd$, $∘$” or “[ ], +, $\inl$, $\inr$, $﹔$”.

\room

For categories in which sums and products exist, we define for $f : A → B$ and $g : C → D$,

\begineqns

\eqn{$\star$-Definition}{ f ⋆ g = ⟅ f \composition l, g \composition r⟆ : A ⋆ C → B ⋆ D}

\eqn{$l,r$-Naturality}{ l \composition (f ⋆ g) = f \composition l \landS r \composition (f ⋆ g) = g \composition r }

\eqn{Extensionality}{ ⟅l \composition h, r \composition h⟆ = h}

\eqn{Absorption}{ (f ⋆ g) \composition ⟅h, j⟆ = ⟅f \composition h, g \composition j⟆ }

\eqn{$\star$-Bi-Functoriality}{ \Id ⋆ \Id = \Id \landS (f ⋆ g) \composition (h ⋆ j) = (f \composition h) ⋆ (g \composition j)}

\eqn{Structural Equality}{ ⟅f,g⟆ = ⟅h, j⟆ \equivS f = h \lands g = j }

\eqn{Interchange Rule}{ ⟨[f,g], [h,j]⟩ = [⟨f,h⟩,⟨g,j⟩] }

\endeqns

\room

Notice that the last law above is self-dual.

* Reference

#+LaTeX: {\color{white}.}

[[https://maartenfokkinga.github.io/utwente/mmf92b.pdf][A Gentle Introduction to Category Theory ─ the calculational approach]]
\newline
by [[https://maartenfokkinga.github.io/utwente/][Maarten Fokkinga]]

\vspace{1em}

An excellent introduction to category theory with examples motivated from programming, in-particular
working with sequences. All steps are shown in a calculational style --which Fokkinga
has made [[https://ctan.org/tex-archive/macros/latex/contrib/calculation][available]] for use with LaTeX-- thereby making it suitable for self-study.

\vspace{1em}

Clear, concise, and an illuminating read.

\newpage
Now notes from /Program Design by Calculation/ of J. Oliveira.
* §2, upto §2.3, An Introduction to Pointfree Programming

# Functional programming literally means “programming with functions”.
The main emphasis is on /compositionality/, one of the main advantages
of “thinking functionally”, explaining how to construct new functions
out of other functions using a minimal set of predefined functional
combinators. This leads to a style which is /pointfree/ in the sense that
function descriptions dispense with variables --also known as /points/.

Why do people look for compact notations?
A compact notation leads to shorter documents, less lines of code
in programming, in which patterns are easier to identify and reason
about. Properties can be stated in clear-cut, one-line long equations
which are easy to memorise. 
#
# Backus' FP and Iverson's APL are examples of pointfree programming
# languages; look them up ;-)

The notation “$f : A → B$” focuses on what is relevant about $f$
and can it be regard as a kind of contract:
#+BEGIN_QUOTE
$f$ /commits itself/ to producing a /B/-value provided it is supplied
with an /A/-value.
#+END_QUOTE

Types provide the “glue”, or interface, for putting functions
together to obtain more complex functions.

What do we want functions for?
One wants functions for modelling and reasoning about the
behaviour of real things.

In order to switch between the pointwise and pointfree settings
we need two “bridges”: One lifting equality to the function level
and the other lifting function application.

Two functions $f, g : A → B$ are the same function iff they agree
at the pointwise level:
\eqn{Extensionality}{ f = g \equivS \left(∀ a : A • f\, a = g\, a \right)}

Function application is replaced by the more generic concept of
functional /composition/ suggested by function-arrow chaining:
Whenever we have two functions such that the target type of one
of them, say $g : B ← A$ is the same as the source type of the other,
say $f : C ← B$ then “$f$ after $g$”, their /composite function/,
$f ∘ g : C ← A$ can be defined. It “glues” $f$ and $g$ together,
“sequentially”:
#+BEGIN_EXPORT latex
\eqn{composition-Type}{ 
  C \overset{f}{\longleftarrow} % B \lands 
  B \overset{g}{\longleftarrow} A \impliesS
  C \overset{\;f ∘ g}{\longleftarrow} A
}
#+END_EXPORT

\newpage
* §2.5 Constant functions

Opposite to the identity functions which do not lose any information,
we find functions which lose all, or almost all, information.
Regardless of their input, the output of these functions is always the
same value.

Let $C$ be a nonempty data domain and let $c : C$.
Then we define the /everywhere c/ function as follows, for arbitrary $A$:
#+LaTeX: \eqn{constant-Defn}{\const{c}\, a = c}

The following property defines constant functions at the pointfree level:
#+LaTeX: \eqn{constant-Fusion}{\const{c} ∘ f = \const{c} \}

Constant functions force any difference in behaviour for any two
functions to disappear:
#+LaTeX: \eqn{constant-Equality}{\const{c} ∘ f = \const{c} ∘ g}

In 𝒮ℯ𝓉, we have that composition and application are bridged
explicitly by the constant functions:
#+BEGIN_EXPORT latex
\eqn{𝒮ℯ𝓉-Bridge}{ f ∘ \const{c} = \const{f \, c}}
#+END_EXPORT

* §2.6 Monics and Epics

Identity functions and constant functions are limit points of the 
functional spectrum with respect to information preservation.
All the other functions are in-between: They “lose” some information,
which is regarded as uninteresting for some reason.

How do functions lose information?
Basically in two ways: They may be “blind” enough to confuse different
inputs, by mapping them to the same output, or they may ignore values
of their target. For instance, $\const{c}$ confuses all inputs
by mapping them all onto $c$. Moreover, it ignores all values of its
target apart from $c$.

Functions which do not confuse their inputs are called /monics/:
They are “post-cancellable”:

#+BEGIN_EXPORT latex
\eqn{monic-Defn}{ f \; \mathsf{monic} \equivS 
  \left(∀ h,k \;•\; f ∘ h = f ∘ k \equivs h = k\right)
}
#+END_EXPORT

Functions which do not ignore values of their target are called
/epics/: They are “pre-cancellable”:
#+BEGIN_EXPORT latex
\eqn{epic-Defn}{ f \; \mathsf{epic} \equivS 
  \left(∀ h,k \;•\; h ∘ f = k ∘ f \equivs h = k\right)
}
#+END_EXPORT

Intuitively, $h = k$ on all points of their source precisely when
they are equal on all image points of $f$, since $f$ being epic means 
it outputs all values of their source.

It is easy to check that “the” identity function is monic and epic, 
while any constant function $\const{c}$ is not monic and is only
epic when its target consists only of $c$.

* §2.7 Isos

A function is an /iso/ iff it is invertible:
There is a function /f˘/ called its “inverse” or “converse” such that
\eqn{inverse-Char}{ f ∘ f˘ = \Id \landS f˘ ∘ f = \Id}

To /construct/ $f˘$, we begin by identifying its type which may give
insight into its necessary `shape' --e.g., as a sum or a product--
then we pick one of these equations and try to reduce it as much as possible
until we arrive at a definition of $f˘$, or its `components'. --E.g., the inverse of ~assoc~.
  + E.g., $coassocr = [\Id + \inl, \inr ∘ \inr] : (A + B) + C ≅ A + (B + C)$, its inverse
    /coassocl/ must be of the shape $[x, [y, z]]$ for unknowns $x,y,z$ which can be calculated
    by solving the equation $[x, [y, z]] ∘ coassocr = \Id$ --Do it!

If, for some reason, $f˘$ is found handier than isomorphism $f$
in reasoning, then the following rules can be of help.

\eqn{inverse-Shunting₁}{ f ∘ x = y \equivS x = f˘ ∘ y}
\eqn{inverse-Shunting₂}{ x ∘ f = y \equivS x = y ∘ f˘}

\room

Consequently, 
Isos are necessarily monic and epic, but in general the other way
around is not true.

\room

Isomorphisms are very important because they convert data from one
“format”, say $A$, to another format, say $B$, without losing 
information. So $f$ and $f˘$ are faithful protocols between the two
formats $A$ and $B$. 
*Of course, these formats contain the same “amount” of information although the same data adopts a “different” shape in each of them.*
─c.f. \nameref{SeqPair-is-Pair-Seq}.

\room

Isomorphic data domains are regarded as “abstractly” the same;
then one write $A ≅ B$.

Finally, note that all classes of functions referred to so far
---identities, constants, epics, monics, and isos---
are closed under composition.

* §2.8 Gluing functions which do not compose -- products

Function composition has been presented above as a basis for gluing functions
together in order to build more complex functions. However, not every two functions
can be glued together by composition.

\room

# $f : A ← C$ and $g : B ← C$ 
For instance, functions $A \overset{f}{\leftlongarrow} C \overset{g}{\rightlongarrow} B$
do not compose with each other since the source of one is not the target of the other.

Since $f$ and $g$ share the same source, their outputs can be paired: $c ↦ (f\, c, g\, c)$.
We may think of the operation which pairs the outputs of $f$ and $g$ as a new function
combinator: $⟨f, g⟩ = c ↦ (f\, c, g\, c)$ --read “$f$ /split/ $g$”.

\room

$⟨f, g⟩\, c$ duplicates $c$ so that $f$ and $g$ can be executed in “parallel” on it.

\room

Function $⟨f,g⟩$ keeps the information of both $f$ and $g$ in the same way
Cartesian product $A × B$ keeps the information of $A$ and $B$.
#
So, in the same way that $A$ data or $B$ data can be retrieved from $A × B$ data
via the projections $A \overset{\fst}{\leftlongarrow} A × B \overset{\snd}{\rightlongarrow} B$,
$f$ and $g$ can be retrieved via the same projections:
\eqn{Cancellation}{\fst ∘ ⟨f, g⟩ = f \lands \snd ∘ ⟨f, g⟩ = g}

\room

A /split/ arises wherever two functions do not compose but share the same source.

\room

How do we glue functions that fail such a requisite, say $f : A ← C$ and $g : B ← D$?
We regard their sources as projections of a product:
$A \overset{f ∘ \fst}{\leftlongarrow} C × D \overset{g ∘ \snd}{\rightlongarrow} B$.
Now they have the same source and so the split combinator can be used:
$f × g = (c, d) ↦ (f\, c, g\, d)$.
#
This corresponds to the “parallel” application of $f$ and $g$, each with its /own/ input.

\room

What is the /interplay/ among the functional combinators: Composition, split, product?
The first two relate to each other via the fusion law
\eqn{Fusion}{⟨f, g⟩ ∘ c = ⟨f ∘ c, g ∘ c⟩}
Notice how it looks like the /definition/ of the split operator but all applications have
been lifted to compositions! Woah!
  + Moreover, the absorption property is just the lifting of the pointwise definition! Woah!

All three combinators interact via the ×-absorption property.

The following is self-inverse,
\eqn{swap-Def}{swap = ⟨\snd, \fst⟩ : A × B ≅ B × A}

* §2.9 Gluing functions which do not compose -- coproducts

In the scenario $A \overset{f}{\rightlongarrow} C \overset{g}{\leftlongarrow} B$,
it is clear that the kind of glue we need should make it possible to apply $f$
if the input is from the “$A$ side” or apply $g$ if it is from the “$B$ side”.

We denote this new combinator “either $f$ or $g$”, $[f, g] : A + B → C$, where the values of $A + B$
can be thought of as “copies” of $A$ or $B$ values which are “stamped” with different
tags in order to guarantee that values which are simultaneously in $A$ and $B$ do not
get mixed up.

Duality is of great /conceptual economy/ since everything that can be said of concept /X/
can be rephrased for /co-X/. *That $\composition$-listing provides eloquent evidence of duality.*

Notice that the fusion law,
\eqn{Fusion}{ f ∘ [g, h] = [f ∘ g, f ∘ h]}
Is essentially the definition: If we're in the left `g` then the result is `f` applied to it;
otherwise if we're in the right `h` then the result is `f` applied to it! Woah: From pointwise to pointfree.

* §2.10 Mixing products and coproducts

Any $f : A + B → C × D$ can be expressed alternatively as an /either/
or as a /split/. It turns out that both formats are identical:
The exchange rule.

For example, $undistr  = ⟨[\fst, \fst], \snd + \snd⟩ = [\Id × \inl, \Id × \inr] : (A × B) + (A × C) → A × (B + C)$.

Also, by the exchange rule,
\eqn{Cool-Property}{ [f × g, h × k] \;=\; ⟨ [f, h] ∘ (\fst + \fst), [g, k] ∘ (\snd + \snd)⟩ }
\eqn{Co-cool-Property}{ ⟨f + g, h + k⟩ \;=\; [ ⟨f, h⟩ ; (\inl × \inl), ⟨g, k⟩ ; (\inr × \inr)] }
# Direct proof:
# 
#   [ ⟨f, h⟩ ; (\inl × \inl), ⟨g, k⟩ ; (\inr × \inr)]
# = [ ⟨inl ∘ f , inl ∘ h⟩, ⟨inr ∘ g, inr ∘ k⟩ ]      absorption
# = ⟨ [inl ∘ f, inr ∘ g], [inl ∘ h, inr ∘ k]⟩       exchange
# = ⟨ f + g, h + k ⟩                               defns

Also, since constants ignore their inputs,
\eqn{Exchange-with-constant}{ [⟨ f, \const{k} ⟩, ⟨ g , \const{k} ⟩] = ⟨ [f, g], \const{k} ⟩ }
# Proof:
#
#   [⟨ f, _k_ ⟩, ⟨ g , _k_ ⟩]
# = ⟨ [f, g], [ _k_ , _k_ ]⟩        exchange
# = ⟨ [f, g], _k_ ∘ [ Id, Id ]⟩   fusion
# = ⟨ [f, g], _k_ ⟩               constants

* §2.13 Universal Properties

The compositional combinators put forward so far have been equipped with a concise /set of properties/
which enable programmers to transform programs, reason about them, and perform useful calculations.
This raises a /programming methodology/ which is scientific and stable.

The relevance of universal properties, such as ℱ-Char, is that it offers a way of /solving equations/
of the form $y = ℱ\, x$. For example, can the identity be expressed, or `reflected', using this combinator?
We just solve the equation $\Id = ℱ\, x$ for unknown(s) $x$ by appealing to the universal property.

* §2.14 Guards and McCarthy's conditional

Define
\eqn{?-Defn}{(p?)\, a = \mathsc{If}\, p\, a \,\mathsc{Then}\, \inl\, a \,\mathsc{Else}\, \inr\, a \mathsc{Fi}}

We call $p? : A → A + A$ the /guard/ associated to predicated /p : A → 𝔹/.
The guard is more informative than /p/ alone: It provides information about the outcome of testing /p/
on some input /a/, encoded in terms of the sum injections, $\inl$ for /true/ and $\inr$ for /false/,
without losing the input /a/ itself.

#+BEGIN_EXPORT latex
\begineqns
\eqn{McCarthy-Conditional-Defn}{ p → g,h \quad=\quad [g, h] ∘ p?}

Hence to reason about conditionals one may seek help in the algebra of sums.

\eqn{conditional-Fusion₁}{ f ∘ (p → g, h) \quad=\quad p → f ∘ h, f ∘ h}

\eqn{conditional-Fusion₂}{ (p → g, h) ∘ k \quad=\quad p ∘ k → g ∘ k, h ∘ k}

\eqn{conditional-product-Fusion}{ k ∘ ⟨ (p → f,h) , (p → g, i) ⟩  \eqs p → k ∘ ⟨f, g⟩ , k ∘ ⟨h, i⟩ }

%
%   k ∘ ⟨ (p → f,h) , (p → g, i) ⟩ 
% = k ∘ ⟨ [f,h] ∘ p? , [g, i] ∘ p? ⟩
% = k ∘ ⟨ [f,h] , [g, i] ⟩ ∘ p?
% = k ∘ [ ⟨f, g⟩ , ⟨h, i⟩ ] ∘ p?
% = [ k ∘ ⟨f, g⟩ , k ∘ ⟨h, i⟩ ] ∘ p?
% = p → k ∘ ⟨f, g⟩ , k ∘ ⟨h, i⟩

\eqn{conditional-Abides-Distributivity}{ ⟨ (p → f,h) , (p → g, i) ⟩  \eqs p → ⟨f, g⟩ , ⟨h, i⟩ }

\eqn{conditional-Idempotency}{p → f,f \eqs f}

\eqn{conditional-⟨⟩-Distributivity}{ ⟨ f, (p → g, h)⟩ \quad=\quad p → ⟨f, g⟩, ⟨f, h⟩ }

\eqn{conditional-×-Distributivity}{ (p → g, h) × f \quad=\quad p ∘ \fst → g × f, h × f}

\endeqns
#+END_EXPORT

\room

*As well as their duals!*

* Wow --another example-- involving conditionals
Just as in the case of proving /tails/ is a natural transformation *without* using
any implementing definitions, here's a similar example.

\room

Here's an illustration of how /smart/ pointfree algebra can be in reasoning about
functions that /one does not actually defined explicitly/. 

It also shows how relevant the /natural properties/ are.

The issue is that our definition of a guard, \ref{?-Defn}, is pointwise and most
likely unsuitable to prove facts such as, for instance,
\[ p? ∘ f = (f + f) ∘ (p ∘ f)? \]
Thinking better, instead of `inventing' \ref{?-Defn}, we might --and perhaps should!--
have defined 
\eqn{two-Defn}{2  \;≅\; 1 + 1}
\eqn{double-Defn}{2 × A \;≅\; A + A}
\eqn{?-Defn-Pointfree}{ p? = double ∘ ⟨p, \Id⟩ }

which actually express rather closely our strategy of switching from products
to coproducts in the definition of $(p?)$.

In particular, *we do not need to define /double/ explicitly*.
From its type we immediately infer its natural, or free, property:
\[ double ∘ (\Id × f) \eqs (f + f) ∘ double \]

It turns out that this is the /knowledge/ we need about $double$
in order to prove the above mentioned property.

\room

The less one has to write to solve a problem, the better.
One saves time and one's brain, adding to productivity.
This is often called /elegance/ when applying a scientific method.

* §2.15 Gluing functions which do not compose --exponentials

Functional application is the bridge between the pointfree and pointwise worlds.

Suppose we are given the task to combine two functions, one binary
$B \overset{f}{\leftlongarrow}{C × A}$ and the other unary $D \overset{g}{\leftlongarrow} A$.
Clearly none of the combinations $f ∘ g, ⟨f, g⟩$, or $[f, g]$ is well-typed. Hence $f$ and $g$
cannot be put together and require some extra interfacing.

\room

Note that $⟨f, g⟩$ would be well-defined in case the $C$ components of $f$'s source
could be somehow “ignored”. Suppose, in fact, that in some particular context
the first argument of $f$ happens to be “irrelevant”, or to be frozen to some $c : C$.
It is easy to derive a new function $f_c : A → B : a ↦ f(c, a)$ from $f$, combines
nicely with $g$ via the split combinator: $⟨f_c , g⟩$ is well-typed and bears the type
$B × D ← A$.

\room

Since $f_c$ denotes a function of type $B ← A$, we will say $f_c : B^A$, where the exponential
is an alternate notation for the arrow --the reason to adopt it is that it is functorial, as will be shown.

We want functions so as to apply them, so it's natural to introduce the /apply/ operator which applies
a function to an argument:
\[ ap : B ← B^A × A \]
\[ ap(f, a) = f \ a \]

#+LaTeX: \def\transpose#1{ \overline{#1} }

#+BEGIN_EXPORT latex
The above “transpose” operation $f ↦ f_c$ is of type $B^A ← C$.
It expresses $f$ as a kind of /C/-indexed family of functions $B ← A$.
It will be denoted by $\transpose{f}$ with the definition $(\transpose{f}\, c)\, a = f(c, a)$.

Observe that $\transpose{f}$ is more tolerant than $f$: The latter is a binary operator and so
requires /both/ arguments $(c,a)$ to become available before application, whereas the former is happy
to be produced with $c$ first  and with $a$ later on.

\eqn{exponential-Char}{ k = \transpose{f} \equivS f = ap ∘ (k × \Id) }

Pointwise, $(ap ∘ (\transpose{f} × \Id))\, (c, a) = ap \, (\transpose{f}\, c, a) = \transpose{f} \, c\, a$
which should be equal to $f(c,a)$ and indeed it is --in pointfree form:

\eqn{exponential-Cancellation}{ f = ap ∘ (\transpose{f} × \Id) }

\eqn{exponential-Id}{ \transpose{ap} = \Id_{B^A} }

\eqn{exponential-Fusion}{ \transpose{g ∘ (f × \Id)} \eqS \transpose{g} ∘ f }

We can thus introduce a new functional combinator, which arises as the transpose
of $f ∘ ap$:

\eqn{exponential-Functor-Type}{ f^A : C^A ← B^A \providedS f : C ← B}

$B^A$ can be thought of as a /syntactical/ representation of the /semantical/ $A → B$
--they are the `same', but the way we treat them and think of them is different.
[ This differs from $(-→-)$ when discussing adjunctions earlier, since that was
  the `external hom-/set/'; this is the /internal/ hom! ]

\eqn{exponential-Functor-Defn}{ f^A = \transpose{f ∘ ap} }
#+END_EXPORT

Pointwise: $f^A\, g = f ∘ g$ --so $f^A$ is the “compose with $f$” functional combinator.
# Indeed:
# 
#     fᵃ = T(f . ap)
# ≡   f . ap = ap ∘ (fᵃ × Id)                 exp-Char
# ≡   f . ap (g, a) = ap ∘ (fᵃ × Id) (g, a)   extensionality
# ≡   f (g a)       = ap ∘ (fᵃ g, a)          ×-simpl
# ≡   f (g a)       = fᵃ g a                  ap-simpl
# ≡   f ∘ g         = fᵃ g                    extensionality

\eqn{exponential-Functorial}{ (g ∘ h)^A \eqs g^A ∘ h^A \landS \Id^A = \Id }

*Interestingly:*

#+BEGIN_EXPORT latex
\eqn{Oh₀}{ \transpose{\snd} \eqs \const{\Id} }

\eqn{Oh₁}{ \transpose{f}\, a \eqs f ∘ ⟨ \constant{a} , \Id ⟩ }

\eqn{Oh₂}{ \const{g} \eqs \transpose{g ∘ \snd} }

Also $\transpose{\snd}$ is a constant function:
\eqn{Oh₃}{ \transpose{\snd} ∘ f \eqs \transpose{\snd} }
#+END_EXPORT


* §2.17 Initial and terminal datatypes

All properties studied for binary /splits/ and binary /eithers/ extend to the
finitary case. For the particular situation $n = 1$, we will have $⟨f⟩=[f]=f$
and $\inl = \fst = \Id$, of course. 

For the particular situation $n = 0$,
finitary products “degenerate” to 1 and finitary sums “degenerate” to 0.
The standard notation for the empty split $⟨⟩$ is $!_C$, where $C$ is the source.
Dually, the standard notation for the empty either $[]$ is $?_C$.

Exercise 2.30: Particularise the exchange law to empty products and empty coproducts; i.e., 1 and 0!

* FIX

0. §2.4, last line:  Should be /`natural'/ instead of /'natural'./

0. §2.5, exercise 2.1: Maybe place this at the end of §2.4 since
     it's a problem on identity functions,
     rather than at the end of §2.5 which is on constant functions.

0. §2.6, end: It's easy to see how monics generalise injective functions
   but it's not as easy for the surjective-epic case. I've conjured up
   the following explanation, which might be helpful to include.
   #+BEGIN_QUOTE
 
#+BEGIN_EXPORT latex
\eqn{epic-Defn}{ f \; \mathsf{epic} \equivS 
  \left(∀ h,k \;•\; h ∘ f = k ∘ f \equivs h = k\right)
}
#+END_EXPORT

   Intuitively, $h = k$ on all points of their source precisely when
   they are equal on all image points of $f$, since $f$ being epic
   means it outputs all values of their source.
   #+END_QUOTE

0. §2.7, before equation 2.18:
   I think you meant /pointfree/ not /pointwise/ as clearly 2.18 involves
   no points.

0. §2.13, ×-fusion calculation: In the final hint, perhaps add /after (2.63)/
   so that it is clear which /derived above/ law is being referenced
   --since its name is in the surrounding text and may not be easy to locate.

0. §2.14, page 39, second paragraph: Typo /market/ should be /marked/.
0. §2.14, page 39, diagram: Typos, left arrow should be labelled /g/ not /f/,
   and the right arrow should be labelled /h/ not /g/.

0. §2.14, exercise 2.21: It seems you have the accidental typos /λap/ appearing twice.

0. §2.15, second paragraph: For $f : B ← C × A$, the phrase /f c ∈ B/ is ill-typed
   and it may be incorrect to say it, /f c/, /denotes a value of type B/.

0. §2.15, typing (2.81): The name /ap/ appears on the arrow, perhaps you do not want it appearing 
   twice in one line like that.

0. §2.15, first paragraph after (2.81): Perhaps it may be better to use a definite article:
   /Back to the generic binary operator/ rather than /Back to generic binary operator/.

0. §2.18, page 56: In the definition of type ~Point~, maybe mention that the Haskell ecosystem
   uses distinct name-spaces for types and their constructors whence the overloading of ``Point''
   is non-ambiguous.

0. §2.19, exercises 2.40: You have $\,.\, swap$ but you likely wanted to use a ~\cdot~ to obtain $·\, swap$.

* COMMENT ToDo's
Include:
+ Page 23: Associativity of ~C~ product structs.
+ Page 25: Realisation of sums in ~C~.
+ Pages 31-32: Datatypes, void, unit, bool, maybe.
+ Page 51: Table 2.1 Abstract notation versus programming language data-structures.

+ Go do all the exercises --we have a handy cheat-sheet to help! Let the exercises reinforce the learning!
  - Some of the insightful, or fun ones, maybe include here ;-)
  - §2.19 is all exercises!

* COMMENT ∞ Further Reads

+ Roland Backhouse
+ Grant Malcolm
+ Lambert Meertens
+ Jaap van der Woude

+ /Adjunctions/ by Fokkinga and Meertens

+ Backus' FP and Iverson's APL are examples of pointfree programming
  languages; look them up ;-)


* newpage :ignore:
  \newpage
* COMMENT footer

(find-file "CheatSheet.el")

# Local Variables:
# eval: (org-babel-tangle)
# eval: (load-file "CheatSheet.el")
# compile-command: (my-org-latex-export-to-pdf)
# End:
