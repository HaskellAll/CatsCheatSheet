#+TITLE: Reference Sheet for Elementary Category Theory
# SUBTITLE: Cheat Sheet Template
# DATE: << Spring 2018 >>
# When we don't provide a date, one is provided for us.
#+AUTHOR: [[http://www.cas.mcmaster.ca/~alhassm/][Musa Al-hassy]] @@latex:{\tiny\hspace{5.5em}\url{https://github.com/alhassy/CatsCheatSheet}}@@
#+EMAIL: alhassy@gmail.com
#+DESCRIPTION: This document is written by Musa Al-hassy for his learning in the spring of 2018.
#+STARTUP: hideblocks
#+STARTUP: overview
#+TODO: BEGIN-IGNORE(b) END-IGNORE(e) } | DONE(d)

# Important shortcuts:
# f7 preview changes
# f8 commit each change
# f9 push changes

#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \newunicodechar{﹔}{\ensuremath{\raisebox{0.4ex}{\tiny \,;\,}}}  %% forward composition ﹔

#+LATEX_HEADER: \usepackage{calculation} 

#+INCLUDE: ~/Dropbox/MyUnicodeSymbols.org
#+INCLUDE: CheatSheet/CheatSheetSetup.org

* TODO commit info                                                   :ignore:

#+begin_tiny
  \vspace{-3.5em}
  \hspace{19em} [[https://github.com/alhassy/CatsCheatSheet/blob/984969814031f56f53732419f297e668d94df863/CheatSheet.pdf][~commit 9849698~]]
  \vspace{2em}
#+end_tiny

* LaTeX Setup :ignore:

#+BEGIN_EXPORT latex
\def\providedS{ \qquad\Leftarrow\qquad }

\def\impliesS{ \qquad\Rightarrow\qquad }

\def\landS{ \qquad\land\qquad }
\def\lands{ \quad\land\quad }

\def\equivs{ \quad\equiv\quad}
\def\equivS{ \qquad\equiv\qquad}

\def\begineqns{ \begingroup\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt} }
\def\endeqns{ \endgroup }
% \def\endeqns{ \endgroup \setlength{\belowdisplayskip}{2pt} } % put belowspace back to desired setting
#+END_EXPORT

# See defn-Type, below for an expanded usage; \eqn{name}{formula}
# LaTeX: \setlength{\abovedisplayskip}{5pt} \setlength{\belowdisplayskip}{2pt}
#+LaTeX: \def\eqn#1#2{ \begin{flalign*} #2 && \tag*{#1} \label{#1} \end{flalign*}  }

#+LaTeX: \def\src{\mathsf{src}}
#+LaTeX: \def\tgt{\mathsf{tgt}}
#+LaTeX: \def\Id{\mathsf{Id}}

# LATEX_HEADER: \setlength{\parskip}{1em}
# LaTeX: \setlength{\parskip}{0.5em}

#+LaTeX: \def\room{\vspace{0.5em}}

* Categories

A *category* 𝒞 consists of a collection of “objects” ~Obj 𝒞,~ a collection of
  “morphisms” ~Mor 𝒞~, an operation ~Id~ associating a morphism ~Idₐ : a → a~ to each object ~a~,
  a parallel pair of functions ~src, tgt : Mor 𝒞 → Obj 𝒞~, and a “composition”
  operation ~_﹔_ : ∀{A B C : Obj} → (A → B) → (B → C) → (A → C)~
  where for objects ~X~ and ~Y~ we define the /type/ ~X → Y~
  as follows
\begin{flalign*}
    f : X \to Y \quad\equiv\quad \mathsf{src}\; f = X \;\land\; \mathsf{tgt}\; f = Y 
   &&
   \tag*{type-Definition}
   \label{type-Definition}
\end{flalign*}
 
 Moreover composition is required to be associative with ~Id~ as identity.
# For other approaches see https://tex.stackexchange.com/a/12035/69371
#
# As we can see from \eqref{defn-Type}\ldots
#
#  \vspace{-1em}

Instead of ~src~ and ~tgt~ we can instead assume primitive a ternary relation
~_:_→_~ and regain the operations precisely when the relation is functional
in its last two arguments:
\eqn{type-Unique}{f : A \to B \;\land\; f : A' \to B' \;\implies\; A=A' \;\land\; B=B'}
When this condition is dropped, we obtain a /pre-category/; e.g., the familiar /Sets/
is a pre-category that is usually treated as a category by making morphisms
contain the information about their source and target: ~(A, f, B) : A → B~
rather than just ~f~. 
\newline
 /This is sometimes easier to give, then src and tgt! C.f. Alg(F)./

\room

Here's an equivalence-preserving property that is useful in algebraic calculations,
#+LaTeX: \eqn{Composition}{ f : A → B \lands g : B → A \equivs f﹔g : A → A \lands g﹔f : B → B}

# A categorical statement is an expression built from notations for objects,
# typing, morphisms, composition, and identities by means of the usual logical
# connectives and quantifications and equality.
#

Example Categories.
+ Each digraph determines a category: The objects are the nodes
  and the paths are the morphisms typed with their starting and ending node.
  Composition is catenation of paths and identity is the empty path.
+ Each preorder determines a category: The objects are the elements
  and there is a morphism ~a → b~ named, say, ~(a, b),~ precisely when $a ≤ b$.

\room

Even when morphisms are functions, the objects need not be sets:
Sometimes the objects are /operations/ --with an appropriate definition
of typing for the functions. The categories of F-algebras are an example
of this.


\newpage

* Functors

A *functor* /F : 𝒜 → ℬ/ is a pair of mappings, denoted by one name,
from the objects, and morphisms, of 𝒜 to those of ℬ such that
it respects the categorical structure:

#+BEGIN_EXPORT latex 
{\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

\eqn{functor-Type}{F\, f : F\, A \to_ℬ F\, B \quad\Leftarrow\quad f : A \to_𝒜 B }

\eqn{Functor}{F\, \Id_A \;=\; \Id_{F\, A}} 

\eqn{Functor}{F\, (f ﹔ g) \;=\; F\, f ﹔ F\, g}

}
#+END_EXPORT

\vspace{1em}

The two axioms are equivalent to the single statement that 
/functors distribute over finite compositions, with $\Id$ being the empty composition/
\[ F(f ﹔ \cdots ﹔ g) \;=\; F\, f ﹔ \cdots ﹔ F\, g \]

Use of Functors.
+ In the definition of a category, “objects” are “just things” for which no internal
  structure is observable by categorical means --composition, identities, morphisms, typing.

  Functors form the tool to deal with “structured” objects

  Indeed in 𝒮ℯ𝓉 the aspect of a structure is that it has “constituents”, and that it is possible
  to apply a function to all the individual constituents; this is done by
  /F f : F A → F B/.

+  For example, let /𝑰 A = A × A/ and /𝑰 f = (x, y) ↦ (f x, f y)./
  So 𝑰 is or represents the structure of pairs; /𝑰 A/ is the set of pairs of /A/,
  and /𝑰 f/ is the function that applies /f/ to each constituent of a pair.

  - A /binary operation on A/ is then just a function /𝑰 A → A;/
    in the same sense we obtain “F-ary operations”.

+  Also, /Seq/ is or represents the structure of sequences; /Seq A/ is the structure of sequences
  over /A/, and /Seq f/ is the function that applies /f/ to each constituent of a sequence.

+  Even though /F A/ is still just an object, a thing with no observable internal structure, the
  functor properties enable to exploit the “structure” of /F A/ by allowing us to “apply”
  a /f/ to each “constituent” by using /F f/.

\vspace{1em}

Category $𝒜lℊ(F)$
+ For a functor /F : 𝒜 → 𝒟/, this category has /F-algebras/, /F/-ary operations in 𝒟 as, objects
  -- i.e., objects are 𝒟-arrows $F\, A → A$ --
  and /F/-homomorphisms as morphisms, and it inherits composition and identities from 𝒟.

  #+BEGIN_EXPORT latex 
  {\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

  \eqn{defn-Homomorphism}{f : ⊕ →_F ⊗ \quad\equiv\quad ⊕ ﹔ f = F\, f ﹔ ⊗ }

  \eqn{id-Homomorphism}{ \Id : ⊕ →_F ⊕ }

  \eqn{comp-Homomorhism}{ f ﹔ g : ⊕ →_F ⊙ \qquad\Leftarrow\qquad f : ⊕ →_F ⊗ \;\land\; g : ⊗ →_F ⊙}
  }
  #+END_EXPORT

  Note that category axiom \eqref{unique-Type} is not fulfilled since a function can be
  a homomorphism between several distinct operations. However, we pretend it is a category
  in the way discussed earlier, and so the carrier of an algebra is fully determined by
  the operation itself, so that the operation itself can be considered the algebra.

  #+BEGIN_CENTER
  /\ref{comp-Homomorhism} renders a semantic property as a syntactic condition!/
  #+END_CENTER

\vspace{1em}

+ A *contravariant functor* 𝒞 → 𝒟 is just a functor /𝒞ᵒᵖ → 𝒟ᵒᵖ/.
+ A *bifunctor* from 𝒞 to 𝒟 is just a functor /𝒞² → 𝒟/.

* Naturality

A natural transformation is nothing but a structure preserving map between functors.
“Structure preservation” makes sense, here, since we've seen already that a functor
is, or represents, a structure that objects might have.

\room

As discussed before for the case /F : 𝒞 → 𝒮ℯ𝓉/, each /F A/ denotes a structured set
and /F/ denotes the structure itself.

\room

#+LaTeX: \def\bin{I\!\!I}

For example, $\bin$ is the structure of pairs, /Seq/ is the structure of sequences,
/Seq Seq/ the structure of sequences of sequences, 
$\bin \, Seq$ the structure of pairs of sequences --which is naturally isomorphic
to $Seq \, \bin$ the structure of sequences of pairs!--, and so on.

\room

A “transformation” from structure /F/ to structure /G/ is a family of functions \newline
$η : ∀\{A\} → F\, A → G\, A$; and it is “natural” if each $η_A$ doesn't affect the /constituents/
of the structured elements in /F A/ but only reshapes the structure of the elements,
from an /F/-structure to a /G/-structure.

\vspace{0em}

#+BEGIN_CENTER
/Reshaping the structure by η commutes with subjecting the constituents to an arbitrary morphism./
#+END_CENTER
# That is, $F\, f ﹔ t_B \;=\; t_A ﹔ G\, f$ for all /f : A → B./

\vspace{-2em}
#+LaTeX: \eqn{ntrf-Def}{ η : F →̣ G \quad\equiv\quad ∀ f \,•\, F\, f ﹔ η_{\tgt\, f}\;=\; η_{\src\, f} ﹔ G\, f }

This is `naturally' remembered: Morphism $η_{\tgt\, f}$ has type $F (\tgt\, f) → G(\tgt\, f)$ and therefore
appears at the target side of an occurrence of /f/; similarly $η_{\src\, f}$ occurs at the source side of an /f/.
/Moreover/ since η is a transformation /from/ /F/ to /G/, functor /F/ occurs at the source side of an η
and functor /G/ at the target side.

\room

+ One also says /ηₐ is natural in/ its parameter /a/.

+ If we take $G = \Id$, then natural transformations $F →̣ \Id$ are precisely /F/-homomorphisms.
+ Indeed, a natural transformation is a sort-of homomorphism in that the image of a morphism
  after reshaping is the same as the reshaping of the image.

\room

Example natural transformations
+ /rev : Seq →̣ Seq : [a₁, …, aₙ] ↦ [aₙ, …, a₁]/
  reverses its argument thereby reshaping a sequence structure into a sequence structure without affecting the constituents.

+ /inits : Seq →̣ Seq Seq : [a₁, …, aₙ] ↦ [[], [a₁], ⋯, [a₁, …, aₙ]]/
  yields all initial parts of its argument
  thereby reshaping a sequence structure into a sequence of sequences structure, not affecting
  the constituents of its argument.

\room

#+BEGIN_EXPORT latex
\begineqns

\eqn{ntr-Ftr}{ Jη : JF →̣ JG \providedS η : F →̣ G \quad \text{ where } (Jη)_A ≔ J(η_A) }

\eqn{ntr-Poly}{ ηK : FK →̣ GK  \hspace{-2ex}\providedS η : F →̣ G \quad \text{ where } (ηK)_A ≔ η_{(K\, A) } }

\eqn{ntrf-Id}{ \Id_F : F →̣ F \text{\hspace{12em} where } (\Id_F)_A ≔ \Id_{(F\, A)} }

\eqn{ntrf-Compose}{ ε ﹔ η : F →̣ H \hspace{2ex}\providedS ε : F →̣ G \lands η : G →̣ H 
\\ \text{ where } (ε ﹔ η)_A = ε_A ﹔ η_A
 }

\endeqns
#+END_EXPORT

\room

*Category ℱ𝓊𝓃𝒸(𝒞, 𝒟)*
consists of functors /𝒞 → 𝒟/ as objects and natrual transformations between them as objects.
The identity transformation is indeed an identity for transformation composition, which is associative. 

\room

*Heuristic* To prove $φ = φ₁ ﹔ ⋯ ﹔ φₙ : F →̣ G$ is a natural transformation, it suffices
to show that each $φᵢ$ is a natural transformation.
 + Theorem \eqref{ntrf-Compose} renders proofs of semantic properties to be trivial type checking!
 + E.g., It's trivial to prove /tails = rev ﹔ inits ﹔ Seq rev/ is a natural transformation
   by type checking, but to prove the naturality equation by using the naturality equations of
   /rev/ and /inits/ --no definitions required-- necessitates more writing, and worse: Actual thought! 

* Adjunctions

An adjunction is a particular one-one correspondence between different kinds of
morphisms in different categories.

\room

An *adjunction* consists of two functors $L : 𝒜 → ℬ$ and $R : ℬ → 𝒜$,
as well as two (not necessarily natural!) transformations
$η : \Id → RL$ and $ε : LR → \Id$ such that

\vspace{-1em}

#+BEGIN_EXPORT latex
\eqn{Adjunction}{\text{\tiny Provided $f : A →_𝒜 R B$ and $g : L A →_ℬ B$ }
 \\ f = η_A ﹔ R g \equivS L f ﹔ ε_B = g
}
#+END_EXPORT

Reading right-to-left: In the equation $L f ﹔ ε_B = g$ there is a unique solution to the unknown $f$.
Dually for the other direction.

\room

That is,
/each L-algebra g is uniquely determined --as an L-map followed by an ε-reduce--/
/by its restriction to the adjunction's unit η./

\room

A famous example is “Free ⊣ Forgetful”, e.g. to /define/ the list datatype, for which the above
becomes: Homomorphisms on lists are uniquely determined, as a map followed by a reduce,
by their restriction to the singleton sequences.

\room

We may call $f$ the restriction, or lowering, of $g$ to the “unital case”
and write $f = ⌊g⌋ = η_A ﹔ R g$. Also, we may call $g$ the extension, or raising,
of $f$ to an /L/-homomorphism and write $g = ⌈f⌉ = L f ﹔ ε_B$. The above equivalence
now reads:

#+BEGIN_EXPORT latex
\begineqns

\eqn{Adjunction-Inverse}{f = ⌊g⌋ \equivS ⌈f⌉ = g}

\eqn{lad-Type}{⌊g⌋_{A,B} = η_A ﹔ R g \; : \; A →_𝒜 R B
 \text{ where } g : L A →_ℬ B
 }

\eqn{rad-Type}{⌈f⌉_{A,B} = L f ﹔ ε_B \; : \; L A →_ℬ B
 \text{ where } f : A →_𝒜 R B
 }

\endeqns
#+END_EXPORT

\room
\vspace{1ex} 
Note that ⌈ is like `r' and the argument to ⌈⌉ must involve the /R/-ight adjoint in its type;
# likewise for 
#+LaTeX: {\textbf L}ad takes morphisms involving the {\textbf L}eft adjoint ;)

\room

This equivalence expresses that `lad' $⌊⌋$, from \emph{l}eft \emph{ad}jungate,
and `rad' $⌈⌉$, from \emph{r}ight \emph{ad}jungate, are each other's inverses
and constitute a correspondence between certain morphisms.
/Being a bijective pair, lad and rad are injective, surjective, and undo one another./

\room

We may think of ℬ as having all complicated problems so we abstract
away some difficulties by \emph{r}aising up to a cleaner, simpler, domain
via rad ⌈⌉; we then solve our problem there, then go back \emph{down} to
the more complicated concrete issue via ⌊⌋, lad.
\newline
( E.g., ℬ is the category of monoids, and 𝒜 is the category of sets; $L$ is the list functor. )

# Some useful results,
#+BEGIN_EXPORT latex
\begineqns

\eqn{ntrf-Adj}{\text{The η and ε determine each other and they are \emph{natural} transformations.}}

\vspace{2ex}
“zig-zag laws” The unit has a post-inverse while the counit has a pre-inverse:

\eqn{unit-Inverse}{ \Id = η   ﹔ R ε}

\eqn{Inverse-counit}{ \Id = L η ﹔ ε} 

\vspace{2ex}
The unit and counit can be regained from the adjunction inverses,

\eqn{unit-Def}{ η = ⌊\Id⌋}

\eqn{counit-Def}{ ε = ⌈\Id⌉ }

\vspace{2ex}
Lad and rad themselves are solutions to the problems of interest, \eqref{Adjunction}.

\eqn{lad-Self}{L ⌊g⌋ ﹔ ε = g}

\eqn{rad-Self}{η ﹔ R ⌈f⌉ = f }

\vspace{2ex}
The following laws assert a kind of
monoic-ness for ε and a kind of epic-ness for η.
Pragmatically they allow us to prove an equality
by shifting to a possibly easier equality obligation.

\eqn{lad-Unique}{ η ﹔ R g = η ﹔ R g′ \equivS g = g′}

\eqn{rad-Unique}{L f ﹔ ε \,= L f′ ﹔ ε \,\equivS f = f′}

\vspace{2ex}
Lad and rad are natural transformations in the category $ℱ𝓊𝓃𝒸(𝒜ᵒᵖ × ℬ, 𝒮ℯ𝓉)$ realising
$(L X → Y) ≅ (X → G Y)$ where $X, Y$ are the first and second projection functors
and $(-→-) : 𝒞ᵒᵖ × 𝒞 → 𝒮ℯ𝓉$ is the hom-functor such that $(f → g) h = f ﹔ h ﹔ g$.
\\ By extensionality in 𝒮ℯ𝓉, their naturality amounts to the laws:

\eqn{lad-Fusion}{⌊ L x ﹔ g ﹔   y ⌋ \;=\; x   ﹔ ⌊g⌋ ﹔ R y }

\eqn{rad-Fusion}{⌈   x ﹔ f ﹔ R y ⌉ \;=\; L x ﹔ ⌈f⌉ ﹔   y }

\room
\endeqns
#+END_EXPORT

Also,
+ Left adjoints preserve colimits such as initial objects and sums.
+ Right adjoints preserve limits such as terminal objects and products.

* Skolemisation
# “Up to Isomorphism”

If a property $P$ holds for precisely one class of isomorphic objects,
and for any two objects in the same class there is precisely one
isomorphism from one to the other, then we say that
/the P-object is unique up to unique isomorphism/. 
For example, in 𝒮ℯ𝓉 the one-point set is unique up to a unique isomorphism,
but the two-point set is not.

\room

For example, an object /A/ is ``initial'' iff
$∀ B  \;•\;  ∃₁ f  \;•\;  f : A → B$, and such objects are unique
up to unique isomorphism --prove it!
The formulation of the definition is clear but it's not very well suited for /algebraic manipulation/.

\room

A convenient formulation is obtained by `skolemisation': An assertion of the form
\[ ∀ x \;•\; ∃ y \;•\; R \, x \, y \]
is equivalent to: There's a function ℱ such that
\[ ∀ x, y \;•\; R \, x \, y \;≡\; y = ℱ\, x  \]
In the former formulation it is the existential quantification “$∃ y$” inside the scope of a universal
one that hinders effective calculation. In the latter formulation the existence claim is brought to a
more global level: A reasoning need no longer be interrupted by the declaration and naming of the
existence of a unique $y$ that depends on $x$; it can be denoted just $ℱ\, x$.
As usual, the final universal quantification can be omitted, thus simplifying the formulation once more.

\room

In view of the important role of the various $y$'s, these $y$'s deserve a particular notation that
triggers the reader of their particular properties. We employ bracket notation such as $⦇ x ⦈$
for such $ℱ\, x$: An advantage of the bracket notation is that no extra parentheses are needed
for composite arguments $x$, which we expect to occur often.

\room

The formula /characterising/ $ℱ$ may be called `ℱ-Char' and it immediately give us some results
by truthifying each side, namely `Self' and `Id'. A bit more on the naming:

| Type        | Possibly non-syntactic constraint on notation being well-formed |
| Self        | It, itself, is a solution                                       |
| Id          | How $\Id$ can be expressed using it                             |
| Uniq        | It's problem has a unique solution                              |
| Fusion      | How it behaves wrt composition                                  |
| Composition | How two instances, in full subcategories, compose               |

Note that the last 3 indicate how the concept interacts with the categorical structure:
$=, ﹔, \Id$. Also note that Self says there's at least one solution and Uniq says there is
at most one solution, so together they are equivalent to ℱ-Char --however those two proofs
are usually not easier nor more elegant than a proof of ℱ-Char directly.

* Initiality

# Convenient definition: 
An object /0 is initial/ if there's a mapping $⦇-⦈$, from objects to morphisms,
such that \ref{initial-Char} holds; from which we obtain a host of useful corollaries.
Alternative notations for $⦇ B ⦈$ are $\text{!`}_B$, or $⦇0 → B⦈$ to make the
dependency on 0 explicit.

#+BEGIN_EXPORT latex 
\begineqns

\eqn{initial-Char}{ f : 0 → B \equivS f = ⦇ B ⦈ }

\eqn{initial-Self}{ ⦇ B ⦈ : 0 → B }

\eqn{initial-Id}{ id₀ = ⦇ 0 ⦈ }

\eqn{initial-Uniq}{ f, g : 0 → B \impliesS f = g }

\eqn{initial-Fusion}{ f : B → C \impliesS ⦇ B ⦈ ﹔ f = ⦇ C ⦈ }

\vspace{2ex}
{\tiny Provided objects $B, C$ are both in 𝒜 and ℬ,
which are full subcategories of some category 𝒞:}
\eqn{initial-Compose}{ ⦇A → B⦈_𝒜 ﹔ ⦇ B → C ⦈_ℬ = ⦇ A → C⦈_𝒜 }
%
% Recall: 𝒟 is a full-subcategory of 𝒞 means: 𝒟(x,y) = 𝒞(x,y) for x,y : Obj 𝒟.

\vspace{2ex}
{\tiny Provided 𝒟 is built on top of 𝒞: 𝒟-objects are composite entities in 𝒞 } 
\eqn{initial-Type}{ B \text{ is an object in 𝒟 } \impliesS ⦇ B ⦈ \text{ is a morphism in 𝒞} }

\endeqns
#+END_EXPORT

\vspace{1em}

These laws become much more interesting when the category is built upon another
one and the typing is expressed as one or more equations in the underlying
category. In particular the importance of fusion laws cannot be over-emphasised;
it is proven by a strengthening step of the form
$⦇B⦈ ﹔ f : 0 → C \providedS ⦇B⦈ : 0 → B \lands f : B → C$.

\room

For example, it can be seen that the datatype of sequences is `the' initial object
in a suitable category, and the mediator $⦇-⦈$ captures
“definitions by induction on the structure”! Hence induction arguments
can be replaced by initiality arguments! Woah!

\room

*Proving Initiality* One may prove that an object $0$ is initial by providing
a definition for $⦇-⦈$ and establishing initial-Char. Almost every such
proof has the following format, or a circular implication thereof: For arbitrary /f/ and /B/,

\vspace{2em}
#+begin_calculation latex
    f : A → B
\step[≡]{}
    ⋮
\step[≡]{}
    f = \text{“an expression not involving $f$”}
\step[≡]{ 𝒹ℯ𝒻𝒾𝓃ℯ $⦇ B ⦈$ to be the rhs of the previous equation }
    f = ⦇ B ⦈
#+end_calculation

\vspace{-2em}

* Colimits

#+LaTeX: \def\const#1{ \underline{#1} }

Each colimit is a certain initial object, and each initial object is a certain colimit.

+ A /diagram in 𝒞/ is a functor $D : 𝒟 → 𝒞$.
+ Define the constant functor $\const{C}\, x = C$ for objects $x$ and
  $\const{C}\, f = \Id_C$ for morphisms $f$. For functions $g : A → B$, we define the natural
  transformation $\const{g} = x \mapsto g : \const{A} →̣ \const{B}$.

+ The category $⋁D$, built upon 𝒞, has objects $γ : D →̣ \const{C}$ called “co-cones”, for
  some object $C =: \tgt\, γ$, and a morphism from $γ$ to $δ$ is a 𝒞-morphism $x$ such that $γ ﹔ \const{x} = δ$.
 
+ A /colimit for D/ is an initial object in $⋁ D$; which may or may not exist.

\room

Writing $γ╲-$ for $⦇-⦈$ and working out the definition of co-cone in terms of equations in 𝒞,
  we obtain: /$γ : Obj(⋁D)$ is a colimit for $D$/ if there is a mapping $γ╲-$ such that /╲-Type/ and
/╲-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\backslash$-Type}{ δ \text{ cocone for } D \impliesS γ╲δ : \tgt\, γ → \tgt\,δ}

\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of $\backslash$-Type is met.
The notation $⋯╲δ$ is only senseful if δ is a co-cone for $D$,
like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

\eqn{$\backslash$-Char}{ γ ﹔ \const{x} = δ \equivS x = γ ╲ δ }

\vspace{2ex}
Notice that for given $x : C → C'$ the equation $γ ╲ δ = x$ \underline{defines}
δ, since by $\backslash$-Char that one equation equivales the family of equations
$δ_A = γ_A ﹔ x$. This allows us to define a natural transformation --or `eithers' in
the case of sums-- using a single function \emph{having} the type of the mediating arrow.

\eqn{$\backslash$-Self}{ γ ; \const{γ╲δ} = δ}

\eqn{$\backslash$-Id}{γ╲γ = \Id}

\eqn{$\backslash$-Fusion}{γ╲δ ﹔x = γ╲(δ ﹔ \const{x})}
% Direct Proof:
%
%    γ╲δ ﹔ x = γ ╲ (δ ﹔ _x_)
% ≡  γ ﹔ \const{γ╲δ ﹔ x} = δ ﹔ _x_          ╲-char
% ≡  γ ﹔ \const{γ╲δ} ﹔ _x_ = δ ﹔ _x_          const functor
% ≡  δ ﹔ _x_ = δ ﹔ _x_                         ╲-self
% ≡ true                                        =-reflexivity

\eqn{$\backslash$-Unique}{γ ﹔ \const{x} = γ ﹔ \const{y} \impliesS x = y}

\vspace{2ex}
This expresses that colimits γ have an epic-like property: \\
The component morphisms $γ_A$ are \emph{jointly epic}.

\vspace{2ex}
The following law confirms the choice of notation once more.

\eqn{$\backslash$-Compose}{γ╲δ ﹔ δ╲ε = γ╲ε}

\vspace{2ex}
The next law tells us that functors distribute over the ╲-notation
provided the implicit well-formedness condition that 
$Fγ$ is a colimit holds --clearly this condition is valid when $F$
preserves colimits.

\eqn{$\backslash$-Functor-Dist}{F(γ╲δ) = Fγ ╲ Fδ}

\eqn{$\backslash$-Pre-Functor-Elim}{γF╲δF = γ╲δ}

\endeqns
#+END_EXPORT


\vspace{-1em}

* Limits

Dually, the category $⋀D$ has objects being “cones” $γ : \const{C} →̣ D$ where $C =: \src\, γ$
is a 𝒞-object, and a morphism to $γ$ /from/ $δ$ is a 𝒞-morphism $x$ such that $\const{x} ﹔ γ = δ$.
In terms of 𝒞, /$γ : Obj(⋀ D)$ is a limit for $D$/ if there is a mapping $γ╱-$ such that
the following ╱-Type and ╱-Char hold, from which we obtain a host of corollaries.
As usual, there is the implicit well-formedness condition. 
\ref{/-Unique} expresses that limits γ have an monic-like property:
The component morphisms $γ_A$ are \emph{jointly monic}.
#
# Well-formedness convention: In each law the variables are quantified
# in such a way that the premise of $/$-Type is met.
# The notation $δ╱⋯$ is only senseful if δ is a cone for $D$,
# like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

#+BEGIN_EXPORT latex
\begineqns

\eqn{/-Type}{ δ \text{ cone for } D \impliesS δ╱γ : \src\, δ → \src\,γ}

\eqn{/-Char}{ \const{x} ﹔ γ = δ \equivS x = δ ╱ γ }

\eqn{/-Self}{ \const{δ ╱ γ} ﹔ γ = δ}

\eqn{/-Id}{γ╱γ = \Id}

\eqn{/-Fusion}{x ﹔ δ ╱ γ = (\const{x} ﹔ δ) ╱ γ}

\eqn{/-Unique}{\const{x} ﹔ γ = \const{y} ﹔ γ \impliesS x = y}

\eqn{/-Functor-Dist}{F(δ ╱ γ) = Fδ ╱ Fγ}

\eqn{/-Pre-Functor-Elim}{δF╱γF = δ╱γ}
% Direct proof:
%
%    δF╱γF = δ╱γ
% ≡  δF = (δ ╱ γ) ﹔ γF
% ≡  (δ ╱ γ ﹔ γ)F = (δ ╱ γ) ﹔ γF
% ≡  (δ ╱ γ ﹔ γ)FA = ((δ ╱ γ) ﹔ γF)A
% ≡  δ ╱ γ ; γFA  = δ╱γ ﹔ γFA
% ≡  true
% 
\endeqns
#+END_EXPORT


\vspace{-1em}

* Coequaliser

# $D𝒟 = \left( \overset{A}{•} \overset{\overset{f}{\rightrightarrows}}{g} \overset{B}{•} \right)$.
#
Take $D$ and $𝒟$ as suggested by $D𝒟 = \left( \overset{A}{•} \rightrightarrows^f_g \overset{B}{•} \right)$;
where $f,g : A → B$ are given. Then a cocone δ for $D$ is a two-member family $δ = (q', q)$
with $q' : A → C, q : B → C, C = \tgt\,\delta$ and $\const{C} h ﹔ δ_A = δ_B ﹔ D h$; in-particular
$q' = q ﹔ f = g ﹔ q$ whence $q'$ is fully-determined by $q$ alone.

Let $γ = (p', p) : Obj(⋁D)$ be a colimit for $D$ and write $p╲-$ in-place of $γ╲-$, then the ╲-laws
yield: /$p$ is a coequaliser of $(f,g)$/ if there is a mapping $p╲-$ such that /CoEq-Type/ and
/CoEq-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{CoEq-Type}{ q ﹔ f =  q ﹔ g \impliesS p╲q : \tgt\, p → \tgt\,q}


\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of \ref{CoEq-Type} is met.
The notation $⋯╲q$ is only senseful if $q﹔f=q﹔g$,
like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

\eqn{CoEq-Char}{ p ﹔ x = q \equivS x = p ╲ q }

\eqn{CoEq-Self}{ p ; p╲q = q}

\eqn{CoEq-Id}{p╲p = \Id}

\eqn{CoEq-Fusion}{p╲q ﹔x = p╲(q ﹔ x)}

\eqn{CoEq-Unique}{p ﹔ x = p ﹔ y \impliesS x = y}

\eqn{CoEq-Compose}{p╲q ﹔ q╲r = p╲r}
%%
%% \eqn{?╲-Functor-Dist}{F(γ╲δ) = Fγ ╲ Fδ}
%% 
%% \eqn{?╲-Pre-Functor-Elim}{γF╲δF = γ╲δ}
%%
\endeqns
#+END_EXPORT

\newpage
* Sums
#+LaTeX: \def\inl{\mathsf{inl}} 
#+LaTeX: \def\inr{\mathsf{inr}}

Take $D$ and $𝒟$ as suggested by $D𝒟 = \left( \overset{A}{•} \;\;\; \overset{B}{•} \right)$.
Then a cocone δ for $D$ is a two-member family $δ = (f, g)$ with
$f : A → C$ and $g : B → C$, where $C = \tgt\, δ$.

\room

Let $γ=(\inl, \inr)$ be a colimit for $D$, let $A + B = \tgt\,γ$, and write
$[f, g]$ in-place of $γ╲(f, g)$, then the ╲-laws yield:
/$(\inl, \inr, A+B)$ form a sum of $A$ and $B$/ if there is a mapping $[-,-]$
such that \ref{[]-Type} and \ref{[]-Char} hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{[]-Type}{f : A → C \lands g : B → C \impliesS [f, g] : A + B → C}

\eqn{[]-Char}{ \inl ﹔ x = f \lands \inr ﹔ x = g \equivS x = [f, g] }

\eqn{[]-Cancellation; []-Self}{ \inl ﹔ [f, g] = f \landS \inr ﹔ [f, g] = g}

\eqn{[]-Id}{ [\inl, \inr] = \Id}

\eqn{[]-Unique}{ \inl ﹔ x = \inl ﹔ y \lands \inr ﹔ x = \inr ﹔ y \impliesS x = y}

\eqn{[]-Fusion}{ [f , g] ﹔ x = [f﹔ x, g ﹔ x] }

\vspace{2ex}
The implicit well-formedness condition in the next law is that
$(F\,\inl, F\,\inr, F(A+B))$ form a sum of $F\,A$ and $F\, B$.
%  --which clearly holds if $F$ preserves sums.

\eqn{[]-Functor-Dist}{F \, [f, g]_𝒞 = [F \, f , F \, g]_𝒟 \qquad\text{ where } F : 𝒞 → 𝒟}
%
% f : A → C
% g : B → C
% F[f,g]     : F(A + B) → F C
% [F f, F g] : F A + F B → F C
%
% Types are okay since, by assumption, F(A + B) forms the sum of FA and FB,
% That is, (F A + F B) ≔ F(A + B)
%
% Direct proof: 
% 
%    F[f,g] = [Ff, Fg]
% ≡  F[f,g] ﹔ F inl = F f  ∧ F[f,g] ﹔ F inr = F g
% ≡  F([f,g] ﹔ inl) = F f  ∧ F([f,g] ﹔ inr) = F g
% ⇐ [f,g] ﹔ inl = f  ∧ [f,g] ﹔ inr = g
% ≡  [f,g] = [f,g]
% ≡  true

\endeqns
#+END_EXPORT

* Duality: Sums & Products

#+BEGIN_EXPORT latex
\def\fst{\mathsf{fst}}
\def\snd{\mathsf{snd}}
#+END_EXPORT

#+LaTeX_HEADER: \usepackage{stackengine} 
#
# \topinset{*}{O}{1pt}{}
#
#+LaTeX: \def\composition{ \topinset{﹔}{∘}{0.4pt}{} }

In category theory there are two popular notations for composition,
$f ﹔ g = g ∘ f$, and there are two arrow notations $A → B \;=\; B \leftarrow A$,
known as the “forwards” and “backwards” notations.

\room

Some people prefer one notation and stick with it; however having both in-hand
allows us to say: The /dual/ of a categorical statement formed with 
$﹔,→$ is obtained by syntactically replacing these two with $∘, \leftarrow$ respectively
while leaving variables and $\Id$'s alone.

\room

#+LaTeX: \def\dual{\mathsf{dual}}
#
#
For example, applying this process to sums yields /products/:
\vspace{-0.5em}
#+begin_calculation latex 
   h = ⟨f, g⟩
\step{  We define products as dual to sums }
   h = \dual [f, g]
\step{  dual operation definition }
  \dual\left( h = [f,g] \right)
\step{  []-Char and Leibniz }
  \dual\left( \inl ﹔ h = f  \lands  \inr ﹔ h = g\right) 
\step{  dual operation definition }
  \dual\left(\inl ﹔ h\right) = f  \lands  \dual\left(\inr ﹔ h\right) = g) 
\step{  dual operation definition }
  \dual \text{ } \inl ∘ h = f  \lands  \dual \text{ }  \inr ∘ h = g
\step{  Define: $\fst = \dual\, \inl$, $\snd = \dual\, \inr$ }
  \fst ∘ h = f  \lands  \snd ∘ h = g
\step{  Switch back to ﹔-notation }
   h ﹔ \fst = f  \lands  h ﹔ \snd = g
#+end_calculation

# Dualising the other sum artefacts yields:
/$(\fst, \snd, A × B)$ form a product of A and B/ if there is an operation
$⟨-,-⟩$ satisfying the Char and Type laws below; from which we obtain a host of corollaries.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\langle\rangle$-Type}{f : C → A \lands g : C → B \impliesS ⟨f, g⟩ : C → A × B}

\eqn{$\langle\rangle$-Char}{ x ﹔ \fst = f \lands x ﹔ \snd = g \equivS x = ⟨f, g⟩ }

\eqn{$\langle\rangle$-Cancellation; $\langle\rangle$-Self}{ ⟨f, g⟩ ﹔ \fst = f \landS ⟨f, g⟩ ﹔ \snd = g}

\eqn{$\langle\rangle$-Id}{ ⟨\fst, \snd⟩ = \Id}

\eqn{$\langle\rangle$-Unique}{ x ﹔ \fst = y ﹔\fst  \lands x; \snd = y ﹔ \snd \impliesS x = y}

\eqn{$\langle\rangle$-Fusion}{ x ﹔ ⟨f , g⟩ = ⟨x﹔f, x ﹔ g⟩ }

\eqn{$\langle\rangle$-Functor-Dist}{F \, ⟨f, g⟩_𝒞 = ⟨F \, f , F \, g⟩_𝒟 \qquad\text{ where } F : 𝒞 → 𝒟}

\endeqns
#+END_EXPORT

\room

These are essentially a re-write of the sum laws; let's write the next set of laws only once.

\room

Let the tuple “⟅ ⟆, ⋆, $l$, $r$, $\composition$” be either
“⟨ ⟩, ×, $\fst$, $\snd$, $∘$” or “[ ], +, $\inl$, $\inr$, $﹔$”.

\room

For categories in which sums and products exist, we define for $f : A → B$ and $g : C → D$,

\begineqns

\eqn{$\star$-Definition}{ f ⋆ g = ⟅ f \composition l, g \composition r⟆ : A ⋆ C → B ⋆ D}

\eqn{$l,r$-Naturality}{ l \composition (f ⋆ g) = f \composition l \landS r \composition (f ⋆ g) = g \composition r }

\eqn{Extensionality}{ ⟅l \composition h, r \composition h⟆ = h}

\eqn{Absorption}{ (f ⋆ g) \composition ⟅h, j⟆ = ⟅f \composition h, g \composition j⟆ }

\eqn{$\star$-Bi-Functoriality}{ \Id ⋆ \Id = \Id \landS (f ⋆ g) \composition (h ⋆ j) = (f \composition h) ⋆ (g \composition j)}

\eqn{Structural Equality}{ ⟅f,g⟆ = ⟅h, j⟆ \equivS f = h \lands g = j }

\eqn{Interchange Rule}{ ⟨[f,g], [h,j]⟩ = [⟨f,h⟩,⟨g,j⟩] }

\endeqns

\room

Notice that the last law above is self-dual.

* Reference

#+LaTeX: {\color{white}.}

[[https://maartenfokkinga.github.io/utwente/mmf92b.pdf][A Gentle Introduction to Category Theory ─ the calculational approach]]
\newline
by [[https://maartenfokkinga.github.io/utwente/][Maarten Fokkinga]]

\vspace{1em}

An excellent introduction to category theory with examples motivated from programming, in-particular
working with sequences. All steps are shown in a calculational style --which Fokkinga
has made [[https://ctan.org/tex-archive/macros/latex/contrib/calculation][available]] for use with LaTeX-- thereby making it suitable for self-study.

\vspace{1em}

Clear, concise, and an illuminating read.

* COMMENT ∞ Further Reads

+ Roland Backhouse
+ Grant Malcolm
+ Lambert Meertens
+ Jaap van der Woude

+ /Adjunctions/ by Fokkinga and Meertens

* newpage :ignore:
  \newpage
* COMMENT footer

(find-file "CheatSheet.el")

# Local Variables:
# eval: (org-babel-tangle)
# eval: (load-file "CheatSheet.el")
# compile-command: (my-org-latex-export-to-pdf)
# End:
